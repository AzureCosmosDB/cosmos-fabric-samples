{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c396fda",
   "metadata": {},
   "source": [
    "# Work with Cosmos DB in Microsoft Fabric using the Cosmos DB Spark Connector\n",
    "\n",
    "This notebook demonstrates how to use Spark and the Azure Cosmos DB Spark connector to read, query, analyze, and write data directly to an Azure Cosmos DB for NoSQL account in Microsoft Fabric. The Spark connector connects directly to the Cosmos DB endpoint to perform operations, which is different from using Spark to read mirrored data stored in OneLake.\n",
    "\n",
    "## ðŸ“‹ Prerequisites\n",
    "\n",
    "Before starting, you'll need:\n",
    "- An existing Cosmos DB database in Microsoft Fabric\n",
    "- An existing container with sample data\n",
    "- A Fabric notebook with Spark (Scala) kernel selected\n",
    "- Custom Spark environment with Cosmos DB Spark Connector libraries (configured in Step 1)\n",
    "\n",
    "> **Note**: This tutorial uses the built-in Cosmos DB sample created with a database name of **CosmosSampleDatabase** and a container name of **SampleData**.\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "- **Configure Spark Environment** - Set up custom libraries for Cosmos DB Spark Connector\n",
    "- **Connect to Cosmos DB** - Authenticate and configure connection settings\n",
    "- **Query Data with DataFrames** - Load and filter data using Spark operations\n",
    "- **Use SparkSQL** - Query Cosmos DB using SQL syntax with Catalog API\n",
    "- **Analyze Data** - Work with nested arrays and perform aggregations\n",
    "- **Write Data** - Create new containers and write processed results\n",
    "\n",
    "## ðŸ“š What You'll Build\n",
    "\n",
    "This notebook will guide you through:\n",
    "1. Setting up your Spark environment with required libraries\n",
    "2. Retrieving your Cosmos DB endpoint\n",
    "3. Connecting to Cosmos DB using Spark connector\n",
    "4. Querying and filtering data with DataFrames\n",
    "5. Using SparkSQL for advanced queries\n",
    "6. Analyzing price history data with array operations\n",
    "7. Creating a new container and writing results\n",
    "\n",
    "## âš ï¸ IMPORTANT: Configuration Required\n",
    "\n",
    "**Before running this notebook:**\n",
    "1. Complete the Spark environment setup (see Step 1 below)\n",
    "2. Get your Cosmos DB endpoint from the Fabric portal\n",
    "3. Update the configuration values in the first code cell\n",
    "4. Select **Spark (Scala)** as your notebook kernel\n",
    "5. Attach your custom environment to this notebook\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Configure Your Spark Environment\n",
    "\n",
    "### Download Required Libraries\n",
    "\n",
    "Download the latest Cosmos DB Spark Connector library files from Maven repository (Spark 3.5):\n",
    "\n",
    "1. **azure-cosmos-spark_3-5_2-12-4.41.0.jar**\n",
    "   - https://repo1.maven.org/maven2/com/azure/cosmos/spark/azure-cosmos-spark_3-5_2-12/4.41.0/azure-cosmos-spark_3-5_2-12-4.41.0.jar\n",
    "\n",
    "2. **fabric-cosmos-spark-auth_3-1.1.0.jar**\n",
    "   - https://repo1.maven.org/maven2/com/azure/cosmos/spark/fabric-cosmos-spark-auth_3/1.1.0/fabric-cosmos-spark-auth_3-1.1.0.jar\n",
    "\n",
    "### Create Custom Spark Environment\n",
    "\n",
    "1. **Create a new notebook** and select **Spark (Scala)** as the language\n",
    "2. **Check workspace settings** to ensure you're using **Runtime 1.3 (Spark 3.5)**\n",
    "3. Click the **environment dropdown** â†’ **New environment**\n",
    "4. **Provide an environment name** (e.g., \"CosmosDB-Spark-Environment\")\n",
    "5. **Ensure Runtime 1.3 (Spark 3.5)** is selected\n",
    "6. Choose **Custom Library** from the **Libraries** folder in the left panel\n",
    "7. **Upload the two .jar files** you downloaded\n",
    "8. Click **Save** â†’ **Publish** â†’ **Publish all** â†’ **Publish**\n",
    "9. **Verify** custom libraries show a status of **success**\n",
    "10. **Return to this notebook** and attach the newly created environment:\n",
    "    - Click environment dropdown â†’ **Change environment** â†’ Select your environment\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Retrieve Your Cosmos DB Endpoint\n",
    "\n",
    "### Get Endpoint from Fabric Portal\n",
    "\n",
    "1. Open the **Fabric portal** (https://app.fabric.microsoft.com)\n",
    "2. Navigate to your **existing Cosmos DB database**\n",
    "3. Select the **Settings** option in the menu bar\n",
    "4. Navigate to the **Connection** section\n",
    "5. **Copy the value** of the \"Endpoint for Cosmos DB NoSQL database\" field\n",
    "6. **Update the ENDPOINT variable** in the cell below with your copied value\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd04611",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Configure Cosmos DB connection settings\n",
    "// UPDATE THESE VALUES with your Cosmos DB information\n",
    "\n",
    "// User values for Cosmos DB\n",
    "val ENDPOINT = \"https://YourAccountEndpoint....cosmos.fabric.microsoft.com:443/\"\n",
    "val DATABASE = \"CosmosSampleDatabase\"  // Your Cosmos DB artifact name\n",
    "val CONTAINER = \"SampleData\"  // Your container name\n",
    "\n",
    "// Set configuration settings\n",
    "val config = Map(\n",
    "      \"spark.cosmos.accountendpoint\" -> ENDPOINT,\n",
    "      \"spark.cosmos.database\" -> DATABASE,\n",
    "      \"spark.cosmos.container\" -> CONTAINER,\n",
    "      // auth config options\n",
    "      \"spark.cosmos.accountDataResolverServiceName\" -> \"com.azure.cosmos.spark.fabric.FabricAccountDataResolver\",\n",
    "      \"spark.cosmos.auth.type\" -> \"AccessToken\",\n",
    "      \"spark.cosmos.useGatewayMode\" -> \"true\",\n",
    "      \"spark.cosmos.auth.aad.audience\" -> \"https://cosmos.azure.com/\"\n",
    ")\n",
    "\n",
    "println(\"âœ… Configuration loaded successfully!\")\n",
    "println(s\"ðŸ“ Endpoint: $ENDPOINT\")\n",
    "println(s\"ðŸ—„ï¸  Database: $DATABASE\")\n",
    "println(s\"ðŸ“¦ Container: $CONTAINER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c21bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Query Data from Container Using DataFrames\n",
    "\n",
    "Load OLTP data into a DataFrame to perform basic Spark operations. The Spark connector will infer the schema by sampling existing items in the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ed04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Read Cosmos DB container into a DataFrame\n",
    "val df = spark.read.format(\"cosmos.oltp\")\n",
    "  .options(config)\n",
    "  .option(\"spark.cosmos.read.inferSchema.enabled\", \"true\")\n",
    "  .load()\n",
    "\n",
    "println(\"âœ… Data loaded into DataFrame!\")\n",
    "println(\"ðŸ“Š Showing first 5 rows:\")\n",
    "\n",
    "// Show the first 5 rows of the dataframe\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0492f7",
   "metadata": {},
   "source": [
    "### Understanding the Schema\n",
    "\n",
    "> **Note**: The *SampleData* container contains two different entity types with separate schemas:\n",
    "> - **product** - Contains product information with pricing and inventory\n",
    "> - **review** - Contains customer reviews with ratings\n",
    ">\n",
    "> The `inferSchema` option will detect both schemas within this Cosmos DB container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b31eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Render schema to understand the data structure\n",
    "println(\"ðŸ“‹ DataFrame Schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd77be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Filter Data by Document Type\n",
    "\n",
    "The container has two schemas distinguished by the `docType` property. Let's filter the data to work with each type separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992370d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Filter for products only using the where function\n",
    "val productsDF = df.where(\"docType = 'product'\")\n",
    "\n",
    "println(\"ðŸ›ï¸  Showing product documents:\")\n",
    "productsDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266ef10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Filter products by specific category using the filter function\n",
    "val filteredDF = df\n",
    "  .where(\"docType = 'product'\")\n",
    "  .filter($\"categoryName\" === \"Computers, Laptops\")\n",
    "\n",
    "println(\"ðŸ’» Showing laptop products:\")\n",
    "filteredDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660bd490",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Query Cosmos DB Using SparkSQL\n",
    "\n",
    "Configure the Catalog API to reference and manage Cosmos DB resources using Spark SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e4e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Configure the Catalog API for SparkSQL access\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog\", \"com.azure.cosmos.spark.CosmosCatalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountEndpoint\", config(\"spark.cosmos.accountendpoint\"))\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.auth.type\", \"AccessToken\")\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.useGatewayMode\", \"true\")\n",
    "spark.conf.set(\"spark.sql.catalog.cosmosCatalog.spark.cosmos.accountDataResolverServiceName\", \"com.azure.cosmos.spark.fabric.FabricAccountDataResolver\")\n",
    "\n",
    "println(\"âœ… Catalog API configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6adc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Query data using SparkSQL\n",
    "val queryString = s\"SELECT * FROM cosmosCatalog.$DATABASE.$CONTAINER\"\n",
    "val queryDF = spark.sql(queryString)\n",
    "\n",
    "println(\"ðŸ“Š Query results (first row):\")\n",
    "queryDF.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba7f855",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Analyze Price History with Array Operations\n",
    "\n",
    "This example demonstrates how to work with embedded arrays in JSON documents. We'll:\n",
    "1. Query the container for product data\n",
    "2. Use the `explode` operator to flatten the priceHistory array\n",
    "3. Calculate the lowest price for each product across its history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6625b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Retrieve product data with price history\n",
    "val productPriceMinDF = spark.sql(\n",
    "  \"SELECT \" +\n",
    "  \"  productId, \" +\n",
    "  \"  categoryName, \" +\n",
    "  \"  name, \" +\n",
    "  \"  currentPrice, \" +\n",
    "  \"  priceHistory \" +\n",
    "  \"FROM cosmosCatalog.\" + DATABASE + \".\" + CONTAINER + \" \" +\n",
    "  \"WHERE \" + CONTAINER + \".docType = 'product'\"\n",
    ")\n",
    "\n",
    "// Prepare an exploded result set containing one row for every priceHistory entry\n",
    "val explodedDF = productPriceMinDF\n",
    "   .withColumn(\"priceHistory\", explode(col(\"priceHistory\")))\n",
    "   .withColumn(\"priceDate\", col(\"priceHistory\").getField(\"date\"))\n",
    "   .withColumn(\"newPrice\", col(\"priceHistory\").getField(\"price\"))\n",
    "\n",
    "// Aggregate the lowest price ever recorded in the priceHistory\n",
    "val lowestPriceDF = explodedDF\n",
    "   .filter(col(\"docType\") === \"product\")\n",
    "   .groupBy(\"productId\", \"categoryName\", \"name\")\n",
    "   .agg(min(\"newPrice\").as(\"lowestPrice\"))\n",
    "\n",
    "println(\"ðŸ’° Lowest historical prices by product:\")\n",
    "lowestPriceDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e0023c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Create New Container and Write Data\n",
    "\n",
    "Now we'll create a new container to store our processed results and write the lowest price data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f6d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create a new container named MinPricePerProduct using the Catalog API\n",
    "val NEW_CONTAINER = \"MinPricePerProduct\"\n",
    "\n",
    "val sqlDef = s\"\"\"\n",
    "   |CREATE TABLE IF NOT EXISTS cosmosCatalog.$DATABASE.$NEW_CONTAINER \n",
    "   |USING cosmos.oltp \n",
    "   |TBLPROPERTIES(partitionKeyPath = '/id', autoScaleMaxThroughput = '1000')\n",
    "\"\"\".stripMargin\n",
    "\n",
    "spark.sql(sqlDef)\n",
    "\n",
    "println(s\"âœ… Container '$NEW_CONTAINER' created successfully!\")\n",
    "println(\"ðŸ“‹ Container settings:\")\n",
    "println(\"   - Partition key: /id\")\n",
    "println(\"   - Autoscale throughput: 1000 RU/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "// All documents in Cosmos DB require an 'id' property\n",
    "// Create an id column with the value of productId\n",
    "val ProductsDF = lowestPriceDF.withColumn(\"id\", col(\"productId\"))\n",
    "\n",
    "println(\"ðŸ“Š Prepared data for writing (with id column):\")\n",
    "ProductsDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cfc457",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Configure the Cosmos DB connection for writing to the new container\n",
    "val configWrite = Map(\n",
    "   \"spark.cosmos.accountendpoint\" -> ENDPOINT,\n",
    "   \"spark.cosmos.database\" -> DATABASE,\n",
    "   \"spark.cosmos.container\" -> NEW_CONTAINER,\n",
    "   \"spark.cosmos.write.strategy\" -> \"ItemOverwrite\",\n",
    "   // auth config options\n",
    "   \"spark.cosmos.accountDataResolverServiceName\" -> \"com.azure.cosmos.spark.fabric.FabricAccountDataResolver\",\n",
    "   \"spark.cosmos.auth.type\" -> \"AccessToken\",\n",
    "   \"spark.cosmos.useGatewayMode\" -> \"true\",\n",
    "   \"spark.cosmos.auth.aad.audience\" -> \"https://cosmos.azure.com/\"\n",
    ")\n",
    "\n",
    "println(\"âœ… Write configuration created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f2c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Write the DataFrame to the new container\n",
    "ProductsDF.write\n",
    "  .format(\"cosmos.oltp\")\n",
    "  .options(configWrite)\n",
    "  .mode(\"APPEND\")\n",
    "  .save()\n",
    "\n",
    "println(\"âœ… Data written successfully to MinPricePerProduct container!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3568f6e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Verify the Write Operation\n",
    "\n",
    "Query the new container to validate that it contains the correct data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef56fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Test our write operation worked\n",
    "val queryString = s\"SELECT * FROM cosmosCatalog.$DATABASE.$NEW_CONTAINER\"\n",
    "val queryDF = spark.sql(queryString)\n",
    "\n",
    "println(\"ðŸ“Š Verifying data in new container:\")\n",
    "queryDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a6b205",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Summary\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "âœ… **Configured Spark Environment** - Set up custom libraries for Cosmos DB Spark Connector  \n",
    "âœ… **Connected to Cosmos DB** - Authenticated using Fabric-specific configuration  \n",
    "âœ… **Queried with DataFrames** - Loaded and filtered data using Spark operations  \n",
    "âœ… **Used SparkSQL** - Queried Cosmos DB using SQL syntax with Catalog API  \n",
    "âœ… **Analyzed Arrays** - Worked with nested priceHistory data using explode  \n",
    "âœ… **Performed Aggregations** - Calculated minimum prices across product history  \n",
    "âœ… **Created Containers** - Used Catalog API to create new Cosmos DB containers  \n",
    "âœ… **Wrote Data** - Persisted processed results back to Cosmos DB  \n",
    "\n",
    "### Key Concepts Demonstrated\n",
    "\n",
    "- **Direct Cosmos DB Access** - Using Spark connector vs. mirrored OneLake data\n",
    "- **Schema Inference** - Automatic schema detection for multiple document types\n",
    "- **DataFrame Operations** - Filtering, transformations, and aggregations\n",
    "- **Array Operations** - Using `explode` to work with nested arrays\n",
    "- **Catalog API** - Managing Cosmos DB resources through SparkSQL\n",
    "- **Write Strategies** - Using ItemOverwrite to handle upsert operations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore additional **DataFrame transformations** on your data\n",
    "- Implement **batch processing** patterns for large datasets\n",
    "- Create **analytical queries** using window functions\n",
    "- Build **data pipelines** combining multiple containers\n",
    "- Integrate with **Power BI** for visualization\n",
    "\n",
    "### ðŸ“š Additional Resources\n",
    "\n",
    "- [Azure Cosmos DB Spark Connector Documentation](https://learn.microsoft.com/azure/cosmos-db/nosql/spark-connector)\n",
    "- [Microsoft Fabric Spark Documentation](https://learn.microsoft.com/fabric/data-engineering/)\n",
    "- [Cosmos DB in Fabric Overview](https://learn.microsoft.com/fabric/database/cosmos-db/)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy querying with Cosmos DB and Spark! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
