{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2848b00",
   "metadata": {},
   "source": [
    "# Spark Analytics and Power BI Dashboard\n",
    "\n",
    "## Price-Review Correlation Analysis in Cosmos DB\n",
    "\n",
    "This notebook demonstrates how to analyze the relationship between product pricing and customer review ratings using the provided sample data. You'll discover insights about how price positioning within product categories correlates with customer satisfaction.\n",
    "\n",
    "### üìã Prerequisites & Setup Instructions\n",
    "\n",
    "**Follow these steps in order:**\n",
    "\n",
    "#### Step 1: Create Cosmos DB Container\n",
    "1. **Create or use Cosmos DB artifact** in your Microsoft Fabric workspace:\n",
    "    - Artifact name: `CosmosSampleDatabase` (or your preferred name)\n",
    "    - This will be your database name used later\n",
    "2. **Open Cosmos DB Data Explorer** for your artifact\n",
    "3. **Create a new SampleData container**\n",
    "    - Select SampleData from the **Home tab**\n",
    "\n",
    "### Optionally: Manually create container\n",
    "4. **Create a new container** with these settings:\n",
    "   - Container name: `SampleData` (or your preferred name)\n",
    "   - Partition key: `/categoryName`\n",
    "5. **Upload sample data**: Use the **Upload** button in Data Explorer\n",
    "6. **Select the file**: Navigate to `datasets/fabricSampleData.json` from this repository\n",
    "7. **Verify upload**: Confirm all documents are loaded successfully\n",
    "\n",
    "#### Step 2: Create Lakehouse and Shortcut\n",
    "1. **Create a new Lakehouse** in your Fabric workspace\n",
    "   - Give it a clear name like `cosmos_sample_lakehouse`\n",
    "2. **Create a shortcut to your Cosmos DB**:\n",
    "   - In your lakehouse, go to **Files** or **Tables** section\n",
    "   - Click **New shortcut**\n",
    "   - Select **Microsoft OneLake** ‚Üí **Cosmos DB**\n",
    "   - Choose your Cosmos DB artifact\n",
    "   - Select the container you created in Step 1\n",
    "3. **Note your names** for configuration:\n",
    "   - Lakehouse name: `your_lakehouse_name`\n",
    "   - Cosmos DB artifact/database: `CosmosSampleDatabase` (or what you named it)\n",
    "   - Container/Table name: `SampleData` (or what you named it)\n",
    "\n",
    "#### Step 3: Configure This Notebook\n",
    "1. **Update cell 4** with your actual names:\n",
    "   - `LAKEHOUSE_NAME` = your lakehouse name\n",
    "   - `DATABASE_NAME` = your Cosmos database name  \n",
    "   - `TABLE_NAME` = your container name\n",
    "\n",
    "### ‚ö†Ô∏è IMPORTANT: Microsoft Fabric Environment Required\n",
    "**This notebook MUST be run in Microsoft Fabric - it cannot be executed locally.**\n",
    "- Requires Fabric **PySpark Python** runtime\n",
    "- Uses Fabric-specific **Spark SQL** syntax for lakehouse data access\n",
    "- Requires access to Cosmos DB data through lakehouse shortcuts\n",
    "- Built-in authentication only works within Fabric notebooks\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- **Analyze price-review correlations** across product categories\n",
    "- **Understand category-relative pricing** vs. absolute price comparisons\n",
    "- **Identify value opportunities** and pricing sweet spots\n",
    "- **Generate business insights** from correlation patterns\n",
    "- **Prepare data for Power BI** dashboards\n",
    "\n",
    "### üìä Analysis Approach\n",
    "Instead of using arbitrary price bands (Budget/Premium), this analysis uses **category-relative positioning** to avoid comparing laptop prices to accessory prices. We calculate price percentiles within each category for meaningful insights.\n",
    "\n",
    "### üö® Resource Naming Best Practice\n",
    "**USING CONSISTENT NAMING**: This notebook uses `CosmosSampleDatabase` (no special characters) for the database name, which avoids SQL escaping issues. The helper function still handles names with dashes if you prefer a different naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b3edf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for data analysis and visualization\n",
    "# This cell will only work in Microsoft Fabric notebook environment\n",
    "\n",
    "print(\"üìã Installing packages for Microsoft Fabric notebook environment...\")\n",
    "%pip install pandas plotly\n",
    "\n",
    "print(\"‚úÖ Package installation complete!\")\n",
    "print(\"üö® REMINDER: This notebook only works in Microsoft Fabric, not locally!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68388e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: Update with YOUR Fabric lakehouse and database names\n",
    "# These should match your actual Fabric setup after following the prerequisites\n",
    "\n",
    "# Your lakehouse name in Fabric (where your mirrored data appears)\n",
    "LAKEHOUSE_NAME = \"cosmos_sample_lakehouse\"  # Replace with your lakehouse name\n",
    "\n",
    "# Your mirrored database name (from Cosmos DB mirroring)\n",
    "DATABASE_NAME = \"CosmosSampleDatabase\"  # Replace with your mirrored database name\n",
    "\n",
    "# Table name (should match your Cosmos DB container name)\n",
    "TABLE_NAME = \"SampleData\"  # Usually matches your container name\n",
    "\n",
    "# Helper function to properly escape database names with special characters\n",
    "def escape_identifier(name):\n",
    "    \"\"\"Escape identifiers containing hyphens or special characters for SQL queries\"\"\"\n",
    "    if '-' in name or ' ' in name or any(c in name for c in ['@', '$', '#']):\n",
    "        return f\"`{name}`\"\n",
    "    return name\n",
    "\n",
    "# Create the properly formatted table reference for Spark SQL\n",
    "ESCAPED_DATABASE = escape_identifier(DATABASE_NAME)\n",
    "FULL_TABLE_NAME = f\"{LAKEHOUSE_NAME}.{ESCAPED_DATABASE}.{TABLE_NAME}\"\n",
    "\n",
    "print(\"üìç Configuration Summary:\")\n",
    "print(f\"  Lakehouse: {LAKEHOUSE_NAME}\")\n",
    "print(f\"  Database: {DATABASE_NAME}\")\n",
    "print(f\"  Table: {TABLE_NAME}\")\n",
    "print(f\"  Full table reference: {FULL_TABLE_NAME}\")\n",
    "print(\"\")\n",
    "print(\"‚ö†Ô∏è  UPDATE REQUIRED:\")\n",
    "print(\"Replace the values above with your actual Fabric lakehouse and database names\")\n",
    "print(\"üí° Database names with hyphens are automatically handled with backticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6bb5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to mirrored Cosmos DB data and validate dataset structure\n",
    "print(\"üîÑ Connecting to mirrored Cosmos DB data...\")\n",
    "print(f\"üìç Table reference: {FULL_TABLE_NAME}\")\n",
    "\n",
    "try:\n",
    "    # Test connection and load sample data\n",
    "    test_query = f\"SELECT * FROM {FULL_TABLE_NAME} LIMIT 10\"\n",
    "    df = spark.sql(test_query)\n",
    "    \n",
    "    # Get basic dataset information\n",
    "    row_count_query = f\"SELECT COUNT(*) as total FROM {FULL_TABLE_NAME}\"\n",
    "    row_count = spark.sql(row_count_query).collect()[0]['total']\n",
    "    \n",
    "    print(f\"‚úÖ Connected successfully!\")\n",
    "    print(f\"üìä Total documents: {row_count:,}\")\n",
    "    print(f\"üìã Columns: {df.columns}\")\n",
    "    \n",
    "    # Validate we have the expected data structure for price-review analysis\n",
    "    expected_columns = ['docType', 'currentPrice', 'stars', 'categoryName']\n",
    "    missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"‚ö†Ô∏è Missing expected columns: {missing_columns}\")\n",
    "        print(\"Please verify you're using the provided fabricSampleData.json\")\n",
    "    else:\n",
    "        print(\"‚úÖ Dataset structure validated for price-review analysis\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(f\"\\nüìã Sample data preview:\")\n",
    "    display(df)\n",
    "    \n",
    "    print(\"üéâ Ready to proceed with price-review correlation analysis!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection error: {str(e)}\")\n",
    "    print(\"üí° Please check:\")\n",
    "    print(\"  ‚Ä¢ Lakehouse name matches your Fabric setup\")\n",
    "    print(\"  ‚Ä¢ Database name matches your mirrored Cosmos DB\")\n",
    "    print(\"  ‚Ä¢ Mirroring process has completed\")\n",
    "    print(\"  ‚Ä¢ You have access permissions to the lakehouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce6dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the complete dataset and prepare for price-review analysis\n",
    "print(\"üìä Loading sample data for price-review correlation analysis...\")\n",
    "\n",
    "try:\n",
    "    # Load the full dataset using Spark SQL\n",
    "    df = spark.sql(f\"SELECT * FROM {FULL_TABLE_NAME}\")\n",
    "    \n",
    "    # Get row count for performance optimization decisions\n",
    "    row_count = spark.sql(f\"SELECT COUNT(*) as total FROM {FULL_TABLE_NAME}\").collect()[0]['total']\n",
    "    \n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"üìä Total records: {row_count:,}\")\n",
    "    \n",
    "    # Cache the dataframe for better performance in analysis queries\n",
    "    df.cache()\n",
    "    \n",
    "    # Convert to Pandas for smaller datasets to enable certain visualizations\n",
    "    if row_count < 100000:\n",
    "        df_pandas = df.toPandas()\n",
    "        print(f\"‚úÖ Converted to Pandas DataFrame for enhanced analysis\")\n",
    "    else:\n",
    "        df_pandas = None\n",
    "        print(f\"üìà Large dataset - using Spark for optimal performance\")\n",
    "    \n",
    "    # Register as temporary view for easy SQL queries\n",
    "    df.createOrReplaceTempView(\"sample_data\")\n",
    "    print(f\"‚úÖ Registered as temporary view 'sample_data'\")\n",
    "    \n",
    "    # Quick data validation for price-review analysis\n",
    "    product_count = df.filter(df.docType == 'product').count()\n",
    "    review_count = df.filter(df.docType == 'review').count()\n",
    "    \n",
    "    print(f\"\\nüìã Data Summary for Analysis:\")\n",
    "    print(f\"  Products: {product_count:,}\")\n",
    "    print(f\"  Reviews: {review_count:,}\")\n",
    "    print(f\"  Categories: {df.select('categoryName').distinct().count()}\")\n",
    "    \n",
    "    if product_count > 0 and review_count > 0:\n",
    "        print(f\"‚úÖ Dataset ready for price-review correlation analysis!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Missing required data - check your dataset structure\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {str(e)}\")\n",
    "    print(\"üí° Verify your configuration and lakehouse access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data structure and validate readiness for price-review correlation analysis\n",
    "print(\"üîç Exploring dataset structure for price-review analysis...\")\n",
    "\n",
    "try:\n",
    "    # Document type distribution - should show products and reviews\n",
    "    print(\"üìã Document Type Distribution:\")\n",
    "    doc_types = df.groupBy(\"docType\").count().orderBy(\"count\", ascending=False)\n",
    "    display(doc_types.toPandas())\n",
    "    \n",
    "    # Category breakdown - shows product catalog diversity\n",
    "    print(\"\\nüìà Products and Reviews by Category:\")\n",
    "    category_breakdown = df.groupBy(\"categoryName\", \"docType\").count().orderBy(\"categoryName\", \"docType\")\n",
    "    category_breakdown_pandas = category_breakdown.toPandas()\n",
    "    \n",
    "    # Pivot to show products vs reviews by category\n",
    "    if df_pandas is not None:\n",
    "        category_pivot = category_breakdown_pandas.pivot(index='categoryName', columns='docType', values='count').fillna(0)\n",
    "        print(\"(Products and Reviews per Category)\")\n",
    "        display(category_pivot)\n",
    "    else:\n",
    "        display(category_breakdown_pandas)\n",
    "    \n",
    "    # Validate data completeness for correlation analysis\n",
    "    print(f\"\\nüìä Data Validation for Correlation Analysis:\")\n",
    "    products_with_prices = df.filter((df.docType == 'product') & (df.currentPrice.isNotNull())).count()\n",
    "    reviews_with_ratings = df.filter((df.docType == 'review') & (df.stars.isNotNull())).count()\n",
    "    \n",
    "    print(f\"  Products with prices: {products_with_prices:,}\")\n",
    "    print(f\"  Reviews with ratings: {reviews_with_ratings:,}\")\n",
    "    \n",
    "    if products_with_prices > 0 and reviews_with_ratings > 0:\n",
    "        print(f\"‚úÖ Dataset validated - ready for price-review correlation analysis!\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Missing price or rating data - check dataset completeness\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error exploring data: {str(e)}\")\n",
    "    print(\"üí° Ensure data was loaded correctly in the previous step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a0e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price range analysis across categories using Spark SQL\n",
    "print(\"\udcb0 Price Analysis by Category:\")\n",
    "\n",
    "try:\n",
    "    # Register the DataFrame as a temporary view for SQL queries\n",
    "    df.createOrReplaceTempView(\"sample_data\")\n",
    "    \n",
    "    # Price analysis query using Spark SQL\n",
    "    price_analysis_query = \"\"\"\n",
    "    SELECT \n",
    "        categoryName,\n",
    "        COUNT(DISTINCT id) as product_count,\n",
    "        MIN(currentPrice) as min_price,\n",
    "        AVG(currentPrice) as avg_price,\n",
    "        MAX(currentPrice) as max_price,\n",
    "        STDDEV(currentPrice) as price_std_dev\n",
    "    FROM sample_data \n",
    "    WHERE docType = 'product' \n",
    "      AND currentPrice IS NOT NULL \n",
    "    GROUP BY categoryName \n",
    "    ORDER BY avg_price DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    df_price_analysis = spark.sql(price_analysis_query)\n",
    "    df_price_analysis_pandas = df_price_analysis.toPandas()\n",
    "    \n",
    "    display(df_price_analysis_pandas)\n",
    "    \n",
    "    # Create visualization of price ranges\n",
    "    if df_price_analysis_pandas is not None and len(df_price_analysis_pandas) > 0:\n",
    "        fig = px.bar(\n",
    "            df_price_analysis_pandas,\n",
    "            x='categoryName',\n",
    "            y='avg_price',\n",
    "            title=\"Average Price by Category\",\n",
    "            labels={'avg_price': 'Average Price ($)', 'categoryName': 'Category'},\n",
    "            text='avg_price'\n",
    "        )\n",
    "        fig.update_traces(texttemplate='$%{text:,.0f}', textposition='outside')\n",
    "        fig.update_xaxes(tickangle=45)\n",
    "        fig.show()\n",
    "        \n",
    "        # Price range visualization\n",
    "        fig2 = go.Figure()\n",
    "        \n",
    "        for _, row in df_price_analysis_pandas.iterrows():\n",
    "            fig2.add_trace(go.Bar(\n",
    "                name=row['categoryName'],\n",
    "                x=[row['categoryName']],\n",
    "                y=[row['max_price'] - row['min_price']],\n",
    "                base=[row['min_price']],\n",
    "                text=f\"${row['min_price']:,.0f} - ${row['max_price']:,.0f}\",\n",
    "                textposition='inside'\n",
    "            ))\n",
    "        \n",
    "        fig2.update_layout(\n",
    "            title=\"Price Ranges by Category\",\n",
    "            xaxis_title=\"Category\",\n",
    "            yaxis_title=\"Price ($)\",\n",
    "            showlegend=False\n",
    "        )\n",
    "        fig2.update_xaxes(tickangle=45)\n",
    "        fig2.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in price analysis: {str(e)}\")\n",
    "    print(\"üí° Make sure the data contains product records with currentPrice field\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a502da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category-Relative Price Analysis using Spark SQL\n",
    "# This analysis shows how ratings vary by price position WITHIN each category,\n",
    "# avoiding the problem of comparing laptop prices to accessory prices.\n",
    "\n",
    "print(\"\udcca Category-Relative Price Position Analysis:\")\n",
    "print(\"This analysis shows how ratings vary by price position WITHIN each category,\")\n",
    "print(\"avoiding the problem of comparing laptop prices to accessory prices.\")\n",
    "\n",
    "try:\n",
    "    price_position_query = \"\"\"\n",
    "    WITH ProductReviews AS (\n",
    "        SELECT \n",
    "            p.currentPrice,\n",
    "            r.stars,\n",
    "            p.categoryName,\n",
    "            -- Calculate relative price position within category (0-1 scale)\n",
    "            (p.currentPrice - MIN(p.currentPrice) OVER (PARTITION BY p.categoryName)) / \n",
    "            NULLIF(MAX(p.currentPrice) OVER (PARTITION BY p.categoryName) - MIN(p.currentPrice) OVER (PARTITION BY p.categoryName), 0) as relative_price_position,\n",
    "            -- Calculate price percentile within category\n",
    "            PERCENT_RANK() OVER (PARTITION BY p.categoryName ORDER BY p.currentPrice) as price_percentile\n",
    "        FROM sample_data p\n",
    "        INNER JOIN sample_data r ON p.id = r.productId\n",
    "        WHERE p.docType = 'product' \n",
    "          AND r.docType = 'review'\n",
    "          AND p.currentPrice IS NOT NULL\n",
    "          AND r.stars IS NOT NULL\n",
    "    ),\n",
    "    PricePositions AS (\n",
    "        SELECT *,\n",
    "            CASE \n",
    "                WHEN price_percentile < 0.25 THEN 'Bottom 25% (Lowest Price)'\n",
    "                WHEN price_percentile < 0.50 THEN '25-50% (Below Average)'\n",
    "                WHEN price_percentile < 0.75 THEN '50-75% (Above Average)'\n",
    "                ELSE 'Top 25% (Highest Price)'\n",
    "            END as price_position_category\n",
    "        FROM ProductReviews\n",
    "    )\n",
    "    SELECT \n",
    "        categoryName,\n",
    "        price_position_category,\n",
    "        COUNT(*) as review_count,\n",
    "        AVG(currentPrice) as avg_price,\n",
    "        AVG(CAST(stars as DOUBLE)) as avg_rating,\n",
    "        MIN(CAST(stars as DOUBLE)) as min_rating,\n",
    "        MAX(CAST(stars as DOUBLE)) as max_rating,\n",
    "        STDDEV(CAST(stars as DOUBLE)) as rating_std_dev,\n",
    "        AVG(relative_price_position) as avg_relative_position,\n",
    "        -- Rating distribution\n",
    "        SUM(CASE WHEN stars = 5 THEN 1 ELSE 0 END) as five_star_count,\n",
    "        SUM(CASE WHEN stars = 4 THEN 1 ELSE 0 END) as four_star_count,\n",
    "        SUM(CASE WHEN stars <= 3 THEN 1 ELSE 0 END) as three_or_less_count\n",
    "    FROM PricePositions\n",
    "    GROUP BY categoryName, price_position_category\n",
    "    HAVING COUNT(*) >= 3  -- Ensure statistical significance\n",
    "    ORDER BY categoryName, avg_relative_position\n",
    "    \"\"\"\n",
    "    \n",
    "    df_price_position = spark.sql(price_position_query)\n",
    "    df_price_position_pandas = df_price_position.toPandas()\n",
    "    \n",
    "    print(\"‚úÖ Price position analysis complete!\")\n",
    "    display(df_price_position_pandas)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in price position analysis: {str(e)}\")\n",
    "    print(\"üí° Make sure the data was loaded correctly and temp view 'sample_data' exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Correlation Analysis: Price vs Review Stars by Category\n",
    "print(\"üîç Price-Review Correlation Analysis by Category:\")\n",
    "\n",
    "try:\n",
    "    # Get raw price-review data for proper correlation calculation\n",
    "    raw_correlation_query = \"\"\"\n",
    "    SELECT \n",
    "        p.categoryName,\n",
    "        p.currentPrice,\n",
    "        CAST(r.stars as DOUBLE) as rating\n",
    "    FROM sample_data p\n",
    "    INNER JOIN sample_data r ON p.id = r.productId\n",
    "    WHERE p.docType = 'product' \n",
    "      AND r.docType = 'review'\n",
    "      AND p.currentPrice IS NOT NULL\n",
    "      AND r.stars IS NOT NULL\n",
    "    \"\"\"\n",
    "    \n",
    "    df_raw_correlation = spark.sql(raw_correlation_query)\n",
    "    df_raw_correlation_pandas = df_raw_correlation.toPandas()\n",
    "    \n",
    "    print(\"‚úÖ Raw correlation data loaded!\")\n",
    "    print(f\"üìä Total price-review pairs: {len(df_raw_correlation_pandas):,}\")\n",
    "    \n",
    "    # Calculate proper correlation coefficients by category using individual product-review pairs\n",
    "    correlation_results = []\n",
    "    \n",
    "    for category in df_raw_correlation_pandas['categoryName'].unique():\n",
    "        category_data = df_raw_correlation_pandas[df_raw_correlation_pandas['categoryName'] == category]\n",
    "        \n",
    "        if len(category_data) >= 10:  # Need sufficient data for meaningful correlation\n",
    "            # Calculate Pearson correlation coefficient on individual data points\n",
    "            correlation_coef = category_data['currentPrice'].corr(category_data['rating'])\n",
    "            \n",
    "            correlation_results.append({\n",
    "                'categoryName': category,\n",
    "                'total_reviews': len(category_data),\n",
    "                'unique_products': category_data.groupby('currentPrice')['rating'].count().count(),\n",
    "                'avg_price': category_data['currentPrice'].mean(),\n",
    "                'avg_rating': category_data['rating'].mean(),\n",
    "                'min_price': category_data['currentPrice'].min(),\n",
    "                'max_price': category_data['currentPrice'].max(),\n",
    "                'price_std': category_data['currentPrice'].std(),\n",
    "                'rating_std': category_data['rating'].std(),\n",
    "                'correlation_coefficient': correlation_coef\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    df_correlation_pandas = pd.DataFrame(correlation_results)\n",
    "    \n",
    "    print(\"‚úÖ Correlation analysis complete!\")\n",
    "    display(df_correlation_pandas)\n",
    "    \n",
    "    # Enhanced correlation interpretation with proper thresholds\n",
    "    print(\"\\nüìä Correlation Analysis Results:\")\n",
    "    for _, row in df_correlation_pandas.iterrows():\n",
    "        corr = row['correlation_coefficient']\n",
    "        \n",
    "        # Proper correlation strength interpretation matching data generation patterns\n",
    "        if pd.isna(corr):\n",
    "            strength = \"No data\"\n",
    "            direction = \"\"\n",
    "        elif abs(corr) >= 0.7:\n",
    "            strength = \"Strong\"\n",
    "            direction = \"positive\" if corr > 0 else \"negative\"\n",
    "        elif abs(corr) >= 0.4:\n",
    "            strength = \"Moderate\"\n",
    "            direction = \"positive\" if corr > 0 else \"negative\"\n",
    "        elif abs(corr) >= 0.2:\n",
    "            strength = \"Weak\"\n",
    "            direction = \"positive\" if corr > 0 else \"negative\"\n",
    "        else:\n",
    "            strength = \"Very weak/None\"\n",
    "            direction = \"\"\n",
    "        \n",
    "        direction_text = f\" ({direction})\" if direction else \"\"\n",
    "        print(f\"  ‚Ä¢ {row['categoryName']}: {strength}{direction_text} correlation (r={corr:.3f})\")\n",
    "    \n",
    "    # Visualization with proper correlation\n",
    "    if len(df_correlation_pandas) > 0:\n",
    "        fig = px.scatter(\n",
    "            df_correlation_pandas,\n",
    "            x='avg_price',\n",
    "            y='avg_rating', \n",
    "            size='unique_products',\n",
    "            color='correlation_coefficient',\n",
    "            color_continuous_scale='RdBu',\n",
    "            color_continuous_midpoint=0,\n",
    "            title='Price vs Rating Correlation by Category',\n",
    "            labels={\n",
    "                'avg_price': 'Average Price ($)', \n",
    "                'avg_rating': 'Average Rating (Stars)',\n",
    "                'correlation_coefficient': 'Correlation Coefficient'\n",
    "            },\n",
    "            hover_data=['total_reviews', 'correlation_coefficient']\n",
    "        )\n",
    "        fig.show()\n",
    "        \n",
    "        # Correlation strength distribution\n",
    "        df_correlation_pandas['correlation_strength'] = df_correlation_pandas['correlation_coefficient'].apply(\n",
    "            lambda x: 'Strong (|r|‚â•0.7)' if abs(x) >= 0.7 else\n",
    "                     'Moderate (|r|‚â•0.4)' if abs(x) >= 0.4 else\n",
    "                     'Weak (|r|‚â•0.2)' if abs(x) >= 0.2 else\n",
    "                     'Very weak/None (|r|<0.2)'\n",
    "        )\n",
    "        \n",
    "        strength_counts = df_correlation_pandas['correlation_strength'].value_counts()\n",
    "        fig2 = px.pie(\n",
    "            values=strength_counts.values,\n",
    "            names=strength_counts.index,\n",
    "            title=\"Distribution of Correlation Strengths Across Categories\"\n",
    "        )\n",
    "        fig2.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in correlation analysis: {str(e)}\")\n",
    "    print(\"üí° Make sure the data contains both product and review records with proper relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22637f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Product Analysis: Best and Worst Performers\n",
    "# Identify products that over/under-perform relative to their price point\n",
    "\n",
    "print(\"üèÜ Product Performance Analysis:\")\n",
    "\n",
    "try:\n",
    "    detailed_analysis_query = \"\"\"\n",
    "    WITH ProductStats AS (\n",
    "        SELECT \n",
    "            p.id as product_id,\n",
    "            p.name as product_name,\n",
    "            p.categoryName,\n",
    "            p.currentPrice,\n",
    "            COUNT(r.id) as review_count,\n",
    "            AVG(CAST(r.stars as DOUBLE)) as avg_rating,\n",
    "            STDDEV(CAST(r.stars as DOUBLE)) as rating_consistency\n",
    "        FROM sample_data p\n",
    "        INNER JOIN sample_data r ON p.id = r.productId\n",
    "        WHERE p.docType = 'product' \n",
    "          AND r.docType = 'review'\n",
    "          AND p.currentPrice IS NOT NULL\n",
    "          AND r.stars IS NOT NULL\n",
    "        GROUP BY p.id, p.name, p.categoryName, p.currentPrice\n",
    "        HAVING COUNT(r.id) >= 2  -- Products with at least 2 reviews\n",
    "    ),\n",
    "    CategoryAverages AS (\n",
    "        SELECT \n",
    "            categoryName,\n",
    "            AVG(currentPrice) as category_avg_price,\n",
    "            AVG(avg_rating) as category_avg_rating\n",
    "        FROM ProductStats\n",
    "        GROUP BY categoryName\n",
    "    )\n",
    "    SELECT \n",
    "        ps.product_name,\n",
    "        ps.categoryName,\n",
    "        ps.currentPrice,\n",
    "        ps.avg_rating,\n",
    "        ps.review_count,\n",
    "        ca.category_avg_price,\n",
    "        ca.category_avg_rating,\n",
    "        -- Performance metrics relative to category\n",
    "        (ps.currentPrice - ca.category_avg_price) as price_vs_category,\n",
    "        (ps.avg_rating - ca.category_avg_rating) as rating_vs_category,\n",
    "        -- Value score: High rating, reasonable price relative to category\n",
    "        CASE \n",
    "            WHEN ps.avg_rating > ca.category_avg_rating AND ps.currentPrice <= ca.category_avg_price THEN 'Great Value'\n",
    "            WHEN ps.avg_rating > ca.category_avg_rating AND ps.currentPrice > ca.category_avg_price THEN 'Premium Quality'\n",
    "            WHEN ps.avg_rating <= ca.category_avg_rating AND ps.currentPrice <= ca.category_avg_price THEN 'Budget Option'\n",
    "            ELSE 'Poor Value'\n",
    "        END as value_category\n",
    "    FROM ProductStats ps\n",
    "    JOIN CategoryAverages ca ON ps.categoryName = ca.categoryName\n",
    "    ORDER BY ps.avg_rating DESC, ps.currentPrice ASC\n",
    "    \"\"\"\n",
    "    \n",
    "    df_detailed = spark.sql(detailed_analysis_query)\n",
    "    df_detailed_pandas = df_detailed.toPandas()\n",
    "    \n",
    "    print(\"‚úÖ Product performance analysis complete!\")\n",
    "    \n",
    "    # Show top 15 products\n",
    "    print(\"\\nüèÜ Top 15 Products by Rating:\")\n",
    "    display(df_detailed_pandas.head(15))\n",
    "    \n",
    "    # Value category distribution\n",
    "    if len(df_detailed_pandas) > 0:\n",
    "        value_distribution = df_detailed_pandas['value_category'].value_counts()\n",
    "        print(f\"\\nüìà Value Category Distribution:\")\n",
    "        for category, count in value_distribution.items():\n",
    "            print(f\"  {category}: {count} products\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in detailed product analysis: {str(e)}\")\n",
    "    print(\"üí° Make sure the data was loaded correctly and temp view 'sample_data' exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd870449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization dashboard\n",
    "# Multiple charts showing correlation patterns and category-relative analysis\n",
    "\n",
    "print(\"üìä Creating comprehensive visualization dashboard...\")\n",
    "\n",
    "# Ensure we have pandas DataFrames for visualization\n",
    "try:\n",
    "    # Convert Spark DataFrames to Pandas if needed\n",
    "    if not isinstance(df_correlation_pandas, pd.DataFrame):\n",
    "        print(\"Converting correlation data to Pandas DataFrame...\")\n",
    "        df_correlation_pandas = df_correlation.toPandas()\n",
    "    \n",
    "    if not isinstance(df_price_position_pandas, pd.DataFrame):\n",
    "        print(\"Converting price position data to Pandas DataFrame...\")\n",
    "        df_price_position_pandas = df_price_position.toPandas()\n",
    "    \n",
    "    if not isinstance(df_detailed_pandas, pd.DataFrame):\n",
    "        print(\"Converting detailed analysis data to Pandas DataFrame...\")\n",
    "        df_detailed_pandas = df_detailed.toPandas()\n",
    "    \n",
    "    print(\"‚úÖ Data conversion complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error converting data: {str(e)}\")\n",
    "    print(\"üí° Make sure all previous analysis steps completed successfully\")\n",
    "\n",
    "# Set up the subplot layout\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\n",
    "        'Price vs Rating by Category',\n",
    "        'Price Position vs Rating (Category-Relative)', \n",
    "        'Category Price Distribution',\n",
    "        'Value Categories Distribution'\n",
    "    ],\n",
    "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "           [{\"secondary_y\": False}, {\"type\": \"pie\"}]]\n",
    ")\n",
    "\n",
    "# 1. Scatter plot: Price vs Rating by Category\n",
    "for category in df_correlation_pandas['categoryName'].unique():\n",
    "    category_data = df_correlation_pandas[df_correlation_pandas['categoryName'] == category]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=category_data['avg_price'],\n",
    "            y=category_data['avg_rating'],\n",
    "            mode='markers',\n",
    "            marker=dict(size=category_data['unique_products'], sizemode='diameter', sizeref=2),\n",
    "            name=category,\n",
    "            text=category_data['categoryName'],\n",
    "            hovertemplate='<b>%{text}</b><br>Price: $%{x:,.0f}<br>Rating: %{y:.2f}<br>Products: %{marker.size}<extra></extra>'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# 2. Category-Relative Price Position Analysis\n",
    "# Aggregate price position data for visualization\n",
    "position_ratings = df_price_position_pandas.groupby('price_position_category').agg({\n",
    "    'avg_rating': 'mean',\n",
    "    'review_count': 'sum',\n",
    "    'avg_relative_position': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=position_ratings['price_position_category'],\n",
    "        y=position_ratings['avg_rating'],\n",
    "        name='Avg Rating by Price Position',\n",
    "        showlegend=False,\n",
    "        text=[f'{rating:.2f}' for rating in position_ratings['avg_rating']],\n",
    "        textposition='auto',\n",
    "        hovertemplate='<b>%{x}</b><br>Avg Rating: %{y:.2f}<br>Total Reviews: %{customdata}<extra></extra>',\n",
    "        customdata=position_ratings['review_count']\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Box plot: Price distribution by category\n",
    "fig.add_trace(\n",
    "    go.Box(\n",
    "        y=df_correlation_pandas['avg_price'],\n",
    "        x=df_correlation_pandas['categoryName'],\n",
    "        name='Price Distribution',\n",
    "        showlegend=False,\n",
    "        hovertemplate='<b>%{x}</b><br>Price: $%{y:,.0f}<extra></extra>'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Pie chart: Value categories\n",
    "value_counts = df_detailed_pandas['value_category'].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Pie(\n",
    "        labels=value_counts.index,\n",
    "        values=value_counts.values,\n",
    "        name=\"Value Categories\",\n",
    "        hovertemplate='<b>%{label}</b><br>Count: %{value}<br>%{percent}<extra></extra>'\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"Product Price-Rating Correlation Analysis Dashboard\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Update axes labels\n",
    "fig.update_xaxes(title_text=\"Average Price ($)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Average Rating\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Price Position Within Category\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Average Rating\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Category\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Price ($)\", row=2, col=1)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüéØ Key Insights from Category-Relative Analysis:\")\n",
    "print(\"1. Price positions are calculated WITHIN each category, making comparisons meaningful\")\n",
    "print(\"2. Look for patterns where higher price positions correlate with higher ratings\")\n",
    "print(\"3. Value categories identify products that over/under-perform relative to category pricing\")\n",
    "print(\"4. This approach avoids the problem of comparing laptop prices to accessory prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354d0902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate actionable business insights from price-review correlation analysis\n",
    "print(\"üéØ BUSINESS INSIGHTS FROM PRICE-REVIEW CORRELATION ANALYSIS\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "try:\n",
    "    # 1. Category performance insights\n",
    "    best_correlation = df_correlation_pandas.nlargest(3, 'avg_rating')\n",
    "    print(\"\\nüìà TOP PERFORMING CATEGORIES (by average rating):\")\n",
    "    for _, row in best_correlation.iterrows():\n",
    "        print(f\"  ‚Ä¢ {row['categoryName']}: {row['avg_rating']:.2f} stars\")\n",
    "        print(f\"    Average price: ${row['avg_price']:,.0f}\")\n",
    "\n",
    "    # 2. Value opportunity identification\n",
    "    if 'df_detailed_pandas' in locals() and len(df_detailed_pandas) > 0:\n",
    "        great_value_products = df_detailed_pandas[df_detailed_pandas['value_category'] == 'Great Value']\n",
    "        print(f\"\\nüí∞ VALUE OPPORTUNITIES:\")\n",
    "        print(f\"  ‚Ä¢ {len(great_value_products)} products identified as 'Great Value'\")\n",
    "        print(f\"  ‚Ä¢ These products rate above average while priced at/below category average\")\n",
    "        \n",
    "        if len(great_value_products) > 0:\n",
    "            print(f\"  ‚Ä¢ Average price: ${great_value_products['currentPrice'].mean():,.0f}\")\n",
    "            print(f\"  ‚Ä¢ Average rating: {great_value_products['avg_rating'].mean():.2f} stars\")\n",
    "\n",
    "    # 3. Category-relative pricing insights\n",
    "    if 'df_price_position_pandas' in locals() and len(df_price_position_pandas) > 0:\n",
    "        position_summary = df_price_position_pandas.groupby('price_position_category')['avg_rating'].mean().sort_values(ascending=False)\n",
    "        best_position = position_summary.index[0]\n",
    "        print(f\"\\nüèÜ OPTIMAL PRICE POSITIONING:\")\n",
    "        print(f\"  ‚Ä¢ {best_position}: {position_summary.iloc[0]:.2f} avg rating\")\n",
    "        print(f\"  ‚Ä¢ This shows the optimal price tier within each category\")\n",
    "\n",
    "    # 4. Strategic recommendations based on correlation patterns\n",
    "    print(f\"\\nüéØ STRATEGIC RECOMMENDATIONS:\")\n",
    "    print(f\"  1. Focus on categories with strong price-quality correlation\")\n",
    "    print(f\"  2. Investigate 'Poor Value' products for pricing optimization\")\n",
    "    print(f\"  3. Use 'Great Value' products for competitive positioning\")\n",
    "    print(f\"  4. Position products in optimal price percentiles within categories\")\n",
    "    print(f\"  5. Monitor rating trends to predict pricing pressure\")\n",
    "\n",
    "    # 5. Category-specific insights\n",
    "    print(f\"\\nüìä CATEGORY-SPECIFIC INSIGHTS:\")\n",
    "    for _, row in df_correlation_pandas.iterrows():\n",
    "        correlation_coef = row.get('correlation_coefficient', 0)\n",
    "        \n",
    "        # Use the same correlation interpretation as the main analysis\n",
    "        if abs(correlation_coef) >= 0.7:\n",
    "            correlation_strength = \"Strong\"\n",
    "        elif abs(correlation_coef) >= 0.4:\n",
    "            correlation_strength = \"Moderate\"  \n",
    "        elif abs(correlation_coef) >= 0.2:\n",
    "            correlation_strength = \"Weak\"\n",
    "        else:\n",
    "            correlation_strength = \"Very weak/None\"\n",
    "            \n",
    "        direction = \"positive\" if correlation_coef > 0 else \"negative\" if correlation_coef < 0 else \"\"\n",
    "        direction_text = f\" ({direction})\" if direction and abs(correlation_coef) >= 0.2 else \"\"\n",
    "        \n",
    "        print(f\"  ‚Ä¢ {row['categoryName']}: {correlation_strength}{direction_text} correlation (r={correlation_coef:.3f})\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Price-review correlation analysis complete!\")\n",
    "    print(f\"üìä Use these insights to optimize pricing strategy and product positioning\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error generating insights: {str(e)}\")\n",
    "    print(\"üí° Ensure all analysis steps completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
