{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2cd6cb4-aa38-4341-8a52-337a53f35a77",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Cosmos DB in Fabric\n",
    "\n",
    "## Python SDK Simple Functions Sample Notebook\n",
    "\n",
    "This sample notebook demonstrates foundational operations for working with Cosmos DB in Fabric using the Python SDK. You'll learn essential concepts for building applications that interact with your Cosmos DB data.\n",
    "\n",
    "### What You'll Learn\n",
    "This notebook covers the following key concepts:\n",
    "\n",
    "- **Authentication** - Secure access to Cosmos DB in Fabric using token credentials\n",
    "- **Database & Container Management** - Initialize clients and work with containers\n",
    "- **Querying Data** - Efficient parameterized queries with partition key optimization\n",
    "- **Point Reads** - High-performance document retrieval by ID\n",
    "- **Data Operations** - Create containers and insert documents with different schemas\n",
    "\n",
    "### Prerequisites\n",
    "- **Runtime**: PySpark/Python runtime environment\n",
    "- **Fabric Artifact**: Cosmos DB in Fabric artifact created in your workspace\n",
    "- **Sample Data**: The sample dataset loaded in Cosmos DB Data Explorer\n",
    "\n",
    "\n",
    "### Getting Started\n",
    "Make sure your Cosmos DB in Fabric artifact is created and loaded with sample data from the Data Explorer before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb25c6-80f5-4e97-beff-e40c8ba0c3a9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#Install packages\n",
    "%pip install azure-cosmos\n",
    "%pip install azure-core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb19b69",
   "metadata": {},
   "source": [
    "Let's gather the necessary imports and set our Cosmos DB in Fabric endpoint from the artifact settings. The Cosmos database name is the same as the Cosmos artifact name in your workspace. We'll use 'SampleData' as the container name for this notebook sample. \n",
    "\n",
    "[!NOTE] You don't need to specify a database name since each the Fabric endpoint maps to one database automatically. This is shown later as an optional step, just be sure to enable the code for that option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dcab5a-3cdf-4c08-828a-6a7b7f6e33a5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-07T17:39:16.5080964Z",
       "execution_start_time": "2025-10-07T17:39:16.1639894Z",
       "normalized_state": "finished",
       "parent_msg_id": "72c9d951-1c8e-4076-bde6-1cadd5f4e252",
       "queued_time": "2025-10-07T17:39:16.1631008Z",
       "session_id": "a533c8c6-00fb-4df8-88ea-c1d0a7cf0082",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Imports and config values\n",
    "import base64, json\n",
    "from typing import Any, Optional\n",
    "\n",
    "#from azure.cosmos.aio import CosmosClient why aio\n",
    "from azure.cosmos import CosmosClient, PartitionKey, ThroughputProperties\n",
    "from azure.core.credentials import TokenCredential, AccessToken\n",
    "\n",
    "\n",
    "COSMOS_ENDPOINT = '' # The Cosmos DB artifact endpoint from the artifact settings\n",
    "COSMOS_DATABASE_NAME = '' # The Cosmos DB artifact name is the database name\n",
    "COSMOS_CONTAINER_NAME = 'SampleData'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce6b86a",
   "metadata": {},
   "source": [
    "The `FabricTokenCredential` class handles secure authentication to Cosmos DB in Fabric by automatically managing token acquisition and refresh for Cosmos DB in Fabric authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a335e2-e135-4ba0-b52e-c20aff19b0cb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-07T17:39:21.6202611Z",
       "execution_start_time": "2025-10-07T17:39:21.2722686Z",
       "normalized_state": "finished",
       "parent_msg_id": "2455ade7-13f7-4f69-b56c-067f4364607c",
       "queued_time": "2025-10-07T17:39:21.2713102Z",
       "session_id": "a533c8c6-00fb-4df8-88ea-c1d0a7cf0082",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Authentication Class\n",
    "\n",
    "class FabricTokenCredential(TokenCredential):\n",
    "    \"\"\"Token credential for Fabric Cosmos DB access with automatic refresh and retry logic.\"\"\"\n",
    "    \n",
    "    def get_token(self, *scopes: str, claims: Optional[str] = None, tenant_id: Optional[str] = None,\n",
    "                  enable_cae: bool = False, **kwargs: Any) -> AccessToken:\n",
    "        access_token = notebookutils.credentials.getToken(\"https://cosmos.azure.com/.default\")\n",
    "        parts = access_token.split(\".\")\n",
    "        if len(parts) < 2:\n",
    "            raise ValueError(\"Invalid JWT format\")\n",
    "        payload_b64 = parts[1]\n",
    "        # Fix padding\n",
    "        padding = (-len(payload_b64)) % 4\n",
    "        if padding:\n",
    "            payload_b64 += \"=\" * padding\n",
    "        payload_json = base64.urlsafe_b64decode(payload_b64.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "        payload = json.loads(payload_json)\n",
    "        exp = payload.get(\"exp\")\n",
    "        if exp is None:\n",
    "            raise ValueError(\"exp claim missing in token\")\n",
    "        return AccessToken(token=access_token, expires_on=exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cd9771",
   "metadata": {},
   "source": [
    "Next, let's initialize all the clients we need to access and modify our Cosmos DB in Fabric artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ffab27-0326-4d4d-9f03-27032280da5f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-10-07T17:39:27.8221704Z",
       "execution_start_time": "2025-10-07T17:39:27.4698859Z",
       "normalized_state": "finished",
       "parent_msg_id": "d0c3e0e1-9623-49d7-89ab-2dab9dbf842a",
       "queued_time": "2025-10-07T17:39:27.4690023Z",
       "session_id": "a533c8c6-00fb-4df8-88ea-c1d0a7cf0082",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize Cosmos DB cosmos client\n",
    "COSMOS_CLIENT = CosmosClient(COSMOS_ENDPOINT, FabricTokenCredential())\n",
    "\n",
    "# [!OPTIONAL] Autoload your database artifact name\n",
    "# In Fabric, the Cosmos DB endpoint maps to one database automatically\n",
    "# This eliminates the need to specify a database name once you provide the endpoint\n",
    "\n",
    "# database_list = list(COSMOS_CLIENT.list_databases())\n",
    "# COSMOS_DATABASE_NAME = database_list[0]['id']\n",
    "\n",
    "# Initialize Cosmos DB database client\n",
    "DATABASE_CLIENT = COSMOS_CLIENT.get_database_client(COSMOS_DATABASE_NAME)\n",
    "\n",
    "# Intialize Cosmos DB container client\n",
    "CONTAINER_CLIENT = DATABASE_CLIENT.get_container_client(COSMOS_CONTAINER_NAME) # Default is SampleData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f889d5ba",
   "metadata": {},
   "source": [
    "Now let's run a query to see some of our sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f355c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query example - Get all Electronics products from our sample data\n",
    "# Since partition key is 'categoryName', this query is efficient and stays within one partition\n",
    "queryText = \"SELECT * FROM c WHERE c.categoryName = @categoryName\"\n",
    "\n",
    "results = CONTAINER_CLIENT.query_items(\n",
    "    query=queryText,\n",
    "    parameters=[\n",
    "        dict(\n",
    "            name=\"@categoryName\",\n",
    "            # docType=\"product\",  # Optional: filter by document type\n",
    "            value=\"Devices, Tablets\"  # Matches the sample data category\n",
    "        )\n",
    "    ],\n",
    "    enable_cross_partition_query=False,  # False since we're filtering by partition key\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"QUERY EXAMPLE\")\n",
    "print(\"Electronics Items Found:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# The query returns all Electronics documents, which may have different structures based on their type.\n",
    "# This demonstrates Cosmos DB's schema flexibility where different document types can coexist in the same container.\n",
    "# We'll handle 'product' and 'customerRating' documents differently since they have unique properties.\n",
    "\n",
    "for item in results:\n",
    "    doc_type = item.get('docType')\n",
    "    \n",
    "    if doc_type == 'product':\n",
    "        # Handle product documents\n",
    "        print(\"üì¶ PRODUCT\")\n",
    "        print(f\"   Product ID: {item.get('productId', 'N/A')}\")\n",
    "        print(f\"   Name: {item.get('name', 'N/A')}\")\n",
    "        print(f\"   Description: {item.get('description', 'N/A')[:100]}...\")\n",
    "        print(f\"   Current Price: ${item.get('currentPrice', 'N/A')}\")\n",
    "        print(f\"   Inventory: {item.get('inventory', 'N/A')} units\")\n",
    "        print(f\"   Country: {item.get('countryOfOrigin', 'N/A')}\")\n",
    "        print(f\"   First Available: {item.get('firstAvailable', 'N/A')}\")\n",
    "        \n",
    "    elif doc_type == 'review':\n",
    "        # Handle customer review documents  \n",
    "        print(\"‚≠ê CUSTOMER REVIEW\")\n",
    "        print(f\"   Product ID: {item.get('productId', 'N/A')}\")\n",
    "        print(f\"   Customer: {item.get('customerName', 'N/A')}\")\n",
    "        print(f\"   Rating: {item.get('stars', 'N/A')}/5 stars\")\n",
    "        print(f\"   Date: {item.get('reviewDate', 'N/A')}\")\n",
    "        # Display first 100 characters of review text if available\n",
    "        review_text = item.get('reviewText', '')\n",
    "        if review_text:\n",
    "            preview = review_text[:100] + \"...\" if len(review_text) > 100 else review_text\n",
    "            print(f\"   Review: {preview}\")\n",
    "        \n",
    "    else:\n",
    "        # Handle unknown document types\n",
    "        print(f\"‚ùì UNKNOWN TYPE: {doc_type}\")\n",
    "        print(f\"   ID: {item.get('id', 'N/A')}\")\n",
    "        \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c15ed1",
   "metadata": {},
   "source": [
    "Now let's run a point read operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e183d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point read example - Get a specific item by ID and partition key (category)\n",
    "# This is the most efficient way to retrieve a single document\n",
    "item_id = \"cb919d62-80e4-4234-9403-b1f272e0c020\"\n",
    "partition_key = \"Devices, Tablets\"  # Using categoryName as partition key\n",
    "\n",
    "# Point read using read_item method\n",
    "item = CONTAINER_CLIENT.read_item(item=item_id, partition_key=partition_key)\n",
    "\n",
    "# Display the results\n",
    "print(\"POINT READ EXAMPLE\")\n",
    "print(f\"   Product ID: {item.get('productId', 'N/A')}\")\n",
    "print(f\"   Name: {item.get('name', 'N/A')}\")\n",
    "print(f\"   Description: {item.get('description', 'N/A')[:100]}...\")\n",
    "print(f\"   Current Price: ${item.get('currentPrice', 'N/A')}\")\n",
    "print(f\"   Inventory: {item.get('inventory', 'N/A')} units\")\n",
    "print(f\"   Country: {item.get('countryOfOrigin', 'N/A')}\")\n",
    "print(f\"   First Available: {item.get('firstAvailable', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d627a",
   "metadata": {},
   "source": [
    "Let's explore adding new data to a seperate container related to this initial product catalog data in SampleData. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040e6d2",
   "metadata": {},
   "source": [
    "Now let's create a new container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea411d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new container with customerId as partition key\n",
    "COSMOS_CONTAINER_NAME2 = \"SampleOrders\"\n",
    "\n",
    "try:\n",
    "    CONTAINER_CLIENT2 = DATABASE_CLIENT.create_container(\n",
    "        id=COSMOS_CONTAINER_NAME2,\n",
    "        partition_key=PartitionKey(path=\"/customerId\"),\n",
    "        offer_throughput=ThroughputProperties(auto_scale_max_throughput=5000)  # Required: Set autoscale throughput or an error will be thrown\n",
    "    )\n",
    "    print(f\"Container {COSMOS_CONTAINER_NAME2} created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating container: {e}\")\n",
    "\n",
    "# List all containers in the database\n",
    "print(f\"Containers in database '{COSMOS_DATABASE_NAME}':\")\n",
    "for container in DATABASE_CLIENT.list_containers():\n",
    "    print(f\"- {container['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacaca29",
   "metadata": {},
   "source": [
    "Now let's create a new item for a customer order within the new container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a customer order document (item) to insert\n",
    "import uuid\n",
    "\n",
    "customer_order = {\n",
    "    \"id\": str(uuid.uuid4()),\n",
    "    \"customerId\": \"cust-456789\", \n",
    "    \"orderDate\": \"2025-10-15T10:30:00\",\n",
    "    \"docType\": \"customerOrder\",\n",
    "    \"status\": \"shipped\",\n",
    "    \"totalAmount\": 1053.54,\n",
    "    \"shippingAddress\": {\n",
    "        \"street\": \"123 Main St\",\n",
    "        \"city\": \"Seattle\", \n",
    "        \"state\": \"WA\",\n",
    "        \"zipCode\": \"98101\"\n",
    "    },\n",
    "    \"items\": [\n",
    "        {\n",
    "            \"productId\": \"a74e7af9-7e13-40cd-90df-7b5e172a8acc\",\n",
    "            \"productName\": \"HyperType Pro K100 RGB Mechanical Keyboard\",\n",
    "            \"quantity\": 2,\n",
    "            \"purchasePrice\": 133.12,\n",
    "            \"categoryName\": \"Peripherals, Keyboards\"\n",
    "        },\n",
    "        {\n",
    "            \"productId\": \"f9619b13-30a7-4ad4-ae00-9817576fba81\",\n",
    "            \"productName\": \"InfinityCore Apex 12 Pro 5G\",\n",
    "            \"quantity\": 1, \n",
    "            \"purchasePrice\": 787.30,\n",
    "            \"categoryName\": \"Devices, Smartphones\"\n",
    "        }\n",
    "    ],\n",
    "    \"paymentMethod\": \"credit_card\",\n",
    "    \"trackingNumber\": \"1Z999AA1234567890\"\n",
    "}\n",
    "\n",
    "# Write the item to the container\n",
    "try:\n",
    "    CONTAINER_CLIENT2.create_item(customer_order)\n",
    "    print(f\"Customer order created successfully!\")\n",
    "    print(f\"Order ID: {customer_order['id']}\")\n",
    "    print(f\"Total: ${customer_order['totalAmount']}\")\n",
    "    print(f\"Status: {customer_order['status']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating order: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61c8fca",
   "metadata": {},
   "source": [
    "Let's read the new order item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474cfe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point read example - Get the customer order by ID and partition key (customerId)\n",
    "order_id = customer_order['id']\n",
    "partition_key = customer_order['customerId']\n",
    "\n",
    "# Point read using read_item method\n",
    "order = CONTAINER_CLIENT2.read_item(item=order_id, partition_key=partition_key)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Order ID: {order['id']}\")\n",
    "print(f\"Customer: {order['customerId']}\")\n",
    "print(f\"Status: {order['status']}\")\n",
    "print(f\"Total: ${order['totalAmount']}\")\n",
    "print(f\"Items: {len(order['items'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e3202",
   "metadata": {},
   "source": [
    "Lastly, let's change the autoscale throughput range for our container. 5000 RU/s is the default autoscale max throughput set for a new container created through the artifact UI. This can be set up to 50,000 RU/s through the SDK. For anything higher please open a support ticket [here](https://app.fabric.microsoft.com/admin-portal/supportCenter) and our team will help you increase your limits further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f11db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current container throughput settings\n",
    "throughput_properties = CONTAINER_CLIENT2.get_throughput()\n",
    "current_autoscale_max = throughput_properties.auto_scale_max_throughput\n",
    "\n",
    "print(f\"Current autoscale max throughput: {current_autoscale_max} RU/s\")\n",
    "\n",
    "# Update container throughput - can be set between 1,000 and 50,000 RU/s for autoscale\n",
    "new_max_throughput = 6000\n",
    "\n",
    "CONTAINER_CLIENT2.replace_throughput(\n",
    "    throughput=ThroughputProperties(\n",
    "        auto_scale_max_throughput=new_max_throughput,\n",
    "        #auto_scale_increment_percent=10  # Optional\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Container throughput updated successfully!\")\n",
    "print(f\"Updated autoscale max throughput: {new_max_throughput} RU/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c24da7f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook sample demonstrates foundational operations for working with Cosmos DB in Fabric using the Python SDK. You've learned how to:\n",
    "\n",
    "- **Authenticate** to Cosmos DB in Fabric using the `FabricTokenCredential` class\n",
    "- **Query data** efficiently using parameterized queries and partition keys\n",
    "- **Perform point reads** for optimal performance and cost\n",
    "- **Create containers** with appropriate partition key strategies\n",
    "- **Insert new documents** and manage different document types\n",
    "- **Manage container throughput** by updating autoscale settings for performance optimization\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore more advanced querying capabilities with aggregations and joins\n",
    "- Learn about indexing strategies for optimal performance\n",
    "- Implement change feed processing for real-time data processing\n",
    "\n",
    "### Resources\n",
    "\n",
    "For comprehensive documentation and tutorials, visit:\n",
    "- **Cosmos DB in Fabric Overview**: [https://learn.microsoft.com/fabric/database/cosmos-db/overview](https://learn.microsoft.com/fabric/database/cosmos-db/overview)\n",
    "- **Python SDK Documentation**: [https://docs.microsoft.com/azure/cosmos-db/sql/sql-api-python-guide](https://docs.microsoft.com/azure/cosmos-db/sql/sql-api-python-guide)"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": null
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": null,
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "43200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
