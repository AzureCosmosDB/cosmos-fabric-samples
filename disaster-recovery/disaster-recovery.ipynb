{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdeff56c",
   "metadata": {},
   "source": [
    "# Disaster Recovery: Restore Cosmos DB Data from OneLake Mirrored Storage\n",
    "\n",
    "This notebook demonstrates how to restore Cosmos DB data in a recovery region by reading mirrored data from OneLake and ingesting it into a recovered Cosmos DB artifact. This is part of a complete disaster recovery strategy using Git integration for artifact configuration and OneLake mirroring for data recovery.\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "Before starting, you'll need to have completed these **disaster recovery preparation steps**:\n",
    "\n",
    "### In the Primary Region (Before Disaster):\n",
    "- ‚úÖ **Git integration enabled** for your Workspace and Cosmos DB artifact\n",
    "- ‚úÖ **Cosmos DB mirroring to OneLake** enabled (automatic, RPO < 15 minutes)\n",
    "- ‚úÖ **Lakehouse created** with shortcuts to OneLake mirrored data\n",
    "- ‚úÖ **OneLake shortcut configured** to access mirrored Cosmos DB containers\n",
    "\n",
    "### In the Recovery Region (After Disaster):\n",
    "- ‚úÖ **New Fabric workspace** created in the recovery region\n",
    "- ‚úÖ **Cosmos DB artifact recovered** from Git repository (configuration only, no data)\n",
    "- ‚úÖ **Custom Spark environment** with Cosmos DB Spark Connector libraries\n",
    "- ‚úÖ **Access to OneLake** in the disaster region (automatic replication)\n",
    "\n",
    "> **Recovery Point Objective (RPO):** < 15 minutes (based on OneLake mirroring)  \n",
    "> **Recovery Time Objective (RTO):** Minutes to hours (depends on data volume)\n",
    "\n",
    "## üéØ What This Notebook Does\n",
    "\n",
    "This notebook will:\n",
    "1. ‚úÖ Read mirrored Cosmos DB data from OneLake Delta tables\n",
    "2. ‚úÖ Connect to the recovered Cosmos DB artifact in the recovery region\n",
    "3. ‚úÖ Ingest data into recovered containers using ItemOverwrite strategy\n",
    "4. ‚úÖ Process multiple containers in a single execution\n",
    "5. ‚úÖ Provide progress logging and record counts\n",
    "\n",
    "## üöÄ Getting Started\n",
    "\n",
    "### Step 1: Import This Notebook to Your Workspace\n",
    "\n",
    "1. Download the `ingest-from-mirror.ipynb` file from the repository\n",
    "2. In your **Fabric workspace** (recovery region), select **Import** ‚Üí **Notebook**\n",
    "3. Upload the downloaded `.ipynb` file\n",
    "4. The notebook will be imported and ready to configure\n",
    "\n",
    "### Step 2: Configure Your Spark Environment\n",
    "\n",
    "Before running this notebook, you must create a custom Spark environment with the required Cosmos DB libraries:\n",
    "\n",
    "1. In your workspace, create **+New item** ‚Üí **Environment**\n",
    "2. Name it (e.g., `CosmosRecoveryEnvironment`)\n",
    "3. Under **Custom libraries**, select **Upload** and add these JAR files:\n",
    "\n",
    "**Required Libraries (Spark 3.5):**\n",
    "- [`azure-cosmos-spark_3-5_2-12-4.41.0.jar`](https://repo1.maven.org/maven2/com/azure/cosmos/spark/azure-cosmos-spark_3-5_2-12/4.41.0/)\n",
    "- [`fabric-cosmos-spark-auth_3-1.0.0.jar`](https://repo1.maven.org/maven2/com/azure/cosmos/spark/fabric-cosmos-spark-auth_3/1.0.0/)\n",
    "\n",
    "4. **Publish** the environment\n",
    "5. In this notebook, select **Settings** ‚Üí **Environment** ‚Üí Select your custom environment\n",
    "6. Wait for the environment to attach (may take a few minutes)\n",
    "\n",
    "### Step 3: Gather Required Information\n",
    "\n",
    "Before running the code, collect the following information:\n",
    "\n",
    "| Parameter | Description | Example |\n",
    "|-----------|-------------|---------|\n",
    "| **disasterRegionWorkspaceName** | Workspace name where OneLake mirrored data resides | `MyWorkspacePrimary` |\n",
    "| **disasterRegionLakehouseName** | Lakehouse name with shortcuts to mirrored data | `CosmosBackupLakehouse` |\n",
    "| **recoveryRegionCosmosAccountEndpoint** | Cosmos DB endpoint in recovery region | `https://abc123.eastus2.sql.cosmos.fabric.microsoft.com:443/` |\n",
    "| **recoveryRegionCosmosDatabase** | Database name in recovered Cosmos DB artifact | `RecoveredDatabase` |\n",
    "| **containerNamesToRecover** | List of container names to restore | `Seq(\"Products\", \"Orders\", \"Customers\")` |\n",
    "\n",
    "**To find your Cosmos DB endpoint:**\n",
    "1. Open your **recovered Cosmos DB artifact** in the recovery region\n",
    "2. Go to **Settings** ‚Üí **Account Endpoint**\n",
    "3. Copy the endpoint URL (includes the artifact ID and region)\n",
    "\n",
    "### Step 4: Update Configuration and Run\n",
    "\n",
    "1. Scroll to the **first code cell** below\n",
    "2. Replace all placeholder values with your actual configuration\n",
    "3. Select the **Spark (Scala)** kernel if not already selected\n",
    "4. Run the notebook cells in order\n",
    "\n",
    "## ‚ö†Ô∏è Important Notes\n",
    "\n",
    "### Data Consistency\n",
    "- **Schema drift**: If property data types changed over time, OneLake may upcast or store nulls\n",
    "- **Hierarchical data**: Arrays and objects are serialized as JSON strings during mirroring\n",
    "- **Deserialization**: You may need to deserialize JSON strings back to structured types\n",
    "\n",
    "### Recovery Strategy\n",
    "- This notebook uses `ItemOverwrite` strategy to handle potential duplicates\n",
    "- Each container is processed sequentially\n",
    "- Progress is logged to help monitor large ingestion operations\n",
    "- Data is persisted in memory to improve write performance\n",
    "\n",
    "### Performance Considerations\n",
    "- Large containers may take significant time to process\n",
    "- Monitor Spark executor logs for progress and errors\n",
    "- Consider batching very large datasets if memory constraints occur\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Related Documentation\n",
    "\n",
    "- [Full Disaster Recovery Guide](./README.md) - Complete BCDR procedures and best practices\n",
    "- [Cosmos DB Spark Connector Documentation](https://github.com/Azure/azure-sdk-for-java/tree/main/sdk/cosmos/azure-cosmos-spark_3_2-12)\n",
    "- [OneLake Shortcuts](https://docs.microsoft.com/fabric/onelake/onelake-shortcuts)\n",
    "- [Git Integration in Fabric](https://docs.microsoft.com/fabric/cicd/git-integration/intro-to-git-integration)\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Configuration Section\n",
    "\n",
    "Update the values below with your specific environment details, then run the cell to begin the recovery process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e502f-462e-43fc-b024-01981f89c6e6",
   "metadata": {
    "microsoft": {
     "language": "scala",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "// ======== USER INPUTS ========\n",
    "\n",
    "// Workspace name in the disaster region\n",
    "val disasterRegionWorkspaceName = \"<DISASTER_REGION_WORKSPACE_NAME>\"  \n",
    "\n",
    "// Lakehouse name in the disaster region\n",
    "val disasterRegionLakehouseName = \"<DISASTER_REGION_LAKEHOUSE_NAME>\"     \n",
    "\n",
    "// Cosmos DB AccountEndpoint in the recovery region \n",
    "// Example: https://<ARTIFACT_ID>.<REGION>.sql.cosmos.fabric.microsoft.com:443/\n",
    "val recoveryRegionCosmosAccountEndpoint = \"<RECOVERY_REGION_COSMOS_ACCOUNT_ENDPOINT>\"  \n",
    "\n",
    "// Cosmos DB Database name in the recovery region \n",
    "val recoveryRegionCosmosDatabase = \"<RECOVERY_REGION_COSMOS_DATABASE>\"  \n",
    "\n",
    "// List of container names to recover\n",
    "val containerNamesToRecover = Seq(\"<CONTAINER_NAME_1>\", \"<CONTAINER_NAME_2>\", \"<CONTAINER_NAME_3>\")\n",
    "\n",
    "containerNamesToRecover.foreach { containerName =>\n",
    "  println(s\"--- Starting BCDR recovery for container: $containerName ---\")\n",
    "\n",
    "  // Construct the OneLake Delta path\n",
    "  val onelakePath =\n",
    "    s\"abfss://${disasterRegionWorkspaceName}@dxt-onelake.dfs.fabric.microsoft.com/${disasterRegionlakehouseName}.Lakehouse/Tables/${containerName}\"\n",
    "\n",
    "  println(s\"Reading Delta table from: $onelakePath\")\n",
    "\n",
    "  // Read from Delta Lakehouse table\n",
    "  val recoveryDF = spark.read.format(\"delta\").load(onelakePath)\n",
    "  recoveryDF.persist()\n",
    "\n",
    "  println(s\"Number of records read: ${recoveryDF.count()}\")\n",
    "\n",
    "  // Cosmos write configuration\n",
    "  val writeCfg = Map(\n",
    "    \"spark.cosmos.auth.type\" -> \"AccessToken\",\n",
    "    \"spark.cosmos.accountEndpoint\" -> recoveryRegionCosmosAccountEndpoint,\n",
    "    \"spark.cosmos.accountDataResolverServiceName\" -> \"com.azure.cosmos.spark.fabric.FabricAccountDataResolver\",\n",
    "    \"spark.cosmos.useGatewayMode\" -> \"true\",\n",
    "    \"spark.cosmos.auth.aad.audience\" -> \"https://cosmos.azure.com/.default\",\n",
    "    \"spark.cosmos.database\" -> recoveryRegioncosmosDatabase, \n",
    "    \"spark.cosmos.container\" -> containerName,\n",
    "    \"spark.cosmos.read.consistencyStrategy\" -> \"LOCAL_COMMITTED\",\n",
    "    \"spark.cosmos.diagnostics\" -> \"sampled\",\n",
    "    \"spark.cosmos.write.strategy\" -> \"ItemOverwrite\"\n",
    "  )\n",
    "\n",
    "  // Write to Cosmos DB Artifact in Recovery Region\n",
    "  println(s\"Writing data to Cosmos container: ${containerName}\")\n",
    "  recoveryDF\n",
    "    .write\n",
    "    .format(\"cosmos.oltp\")\n",
    "    .mode(\"Append\")\n",
    "    .options(writeCfg)\n",
    "    .save()\n",
    "\n",
    "  println(s\" Completed write for container: ${containerName}\\n\")\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
