{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4134709-bfbd-43b0-a0fd-afc8facbc2f2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Cosmos DB in Fabric #\n",
    "# Credit Card Fraud Detection Sample – Part 3: Fraud Detection via Cosmos Change Feed in Spark #\n",
    "This section demonstrates how fraud can be detected through streaming transaction inputs. In the previous notebook we added pending transactions into the Pending Transaction Container in our Cosmos Database. Here we will read the changefeed and process the pending transactions to check if the transaction is fraudulent based on the vector distance of the transaction embedding compared to the transaction history of the card. If it is fraudulent we update the Credit Card Container to lock the card. If it is not fraudulent then we finally add it to the Transaction Container.\n",
    "\n",
    "### Prerequisites\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "- A **Cosmos DB artifact** created in Microsoft Fabric.\n",
    "- Two containers:\n",
    "    - **CCTransactions** – Stores credit card transaction records. \n",
    "        Indexing Policy\n",
    "        {\n",
    "        \"path\": \"/embedding\",\n",
    "        \"type\": \"DiskANN\",\n",
    "        \"dimensions\": 1536,\n",
    "        \"metric\": \"cosine\",\n",
    "        \"quantizationByteSize\": 4,\n",
    "        \"indexingSearchListSize\": 128,\n",
    "        \"vectorIndexShardKey\": [\"/card_id\"]\n",
    "        }\n",
    "        data type: float 32\n",
    "    - **CreditCards** – Stores credit card details.\n",
    "    - **PendingCCTransactions** - Stores pending credit card transactions.\n",
    " \n",
    "- An **OpenAI endpoint and key** for generating embeddings (placeholders will be used in this sample).\n",
    "- Installed required Python packages.\n",
    "- An **Enviroment** set up with the following libraries added:\n",
    "    - [azure-cosmos-spark_3-5_2-12-4.41.0.jar](https://repo1.maven.org/maven2/com/azure/cosmos/spark/azure-cosmos-spark_3-5_2-12/4.41.0/azure-cosmos-spark_3-5_2-12-4.41.0.jar)\n",
    "    - [ fabric-cosmos-spark-auth_3-1.1.0.jar](https://repo1.maven.org/maven2/com/azure/cosmos/spark/fabric-cosmos-spark-auth_3/1.1.0/fabric-cosmos-spark-auth_3-1.1.0.jar)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e917729-8970-4e6f-a1de-8e5d55788bcb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Install the required python modules\n",
    "%pip install azure-core azure-cosmos\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95915598-517b-4f2d-84d4-dd9ec7357622",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Imports and Configuration ###\n",
    "\n",
    "Set up imports and define configuration values for Cosmos DB and OpenAI. Replace placeholder strings with your actual values when running in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9d0e8-7ba7-478f-936f-03c5fbbff9c5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#Imports and config values\n",
    "import base64, json\n",
    "import openai\n",
    "import os\n",
    "import uuid\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from typing import Any, Optional, List, Dict, Tuple\n",
    "\n",
    "#from azure.cosmos.aio import CosmosClient why aio\n",
    "from azure.cosmos import CosmosClient, PartitionKey, ThroughputProperties\n",
    "from azure.core.credentials import TokenCredential, AccessToken\n",
    "\n",
    "\n",
    "COSMOS_ENDPOINT = '<COSMOS_ENDPOINT>' # The Cosmos DB artifact endpoint from the artifact settings\n",
    "COSMOS_DATABASE_NAME = '<COSMOS_DATABASE_NAME>' # The Cosmos DB artifact name is the database name\n",
    "COSMOS_TRANSACTION_CONTAINER_NAME = 'CCTransactions'\n",
    "COSMOS_PENDING_TRANSACTION_CONTAINER_NAME = 'PendingCCTransactions'\n",
    "COSMOS_CC_CONTAINER_NAME = 'CreditCards'\n",
    "\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "OPEN_AI_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "\n",
    "# Spark configs for Cosmos connector using Azure AD (no keys)\n",
    "base_read_cfg = {\n",
    "    \"spark.cosmos.auth.type\" : \"AccessToken\",\n",
    "    \"spark.cosmos.accountEndpoint\": COSMOS_ENDPOINT,\n",
    "    \"spark.cosmos.database\": COSMOS_DATABASE_NAME,\n",
    "    \"spark.cosmos.read.inferSchema.enabled\": \"false\",\n",
    "    \"spark.cosmos.read.consistencyStrategy\" : \"LOCAL_COMMITTED\",\n",
    "    \"spark.cosmos.auth.aad.audience\" : \"https://cosmos.azure.com/.default\",\n",
    "    \"spark.cosmos.accountDataResolverServiceName\" : \"com.azure.cosmos.spark.fabric.FabricAccountDataResolver\",\n",
    "    \"spark.cosmos.useGatewayMode\" : \"true\",\n",
    "    # Optional: app name for diagnostics\n",
    "    \"spark.cosmos.applicationName\": \"pending-to-cc-stream\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eaae49-8361-4721-bf26-1cdac931537b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Authentication Class ###\n",
    "\n",
    "Use a custom credential class to authenticate securely with Cosmos DB using Fabric tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8888396a-03a5-4ed6-b110-c6a2d117132c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "## Authentication Class\n",
    "\n",
    "class FabricTokenCredential(TokenCredential):\n",
    "    \"\"\"Token credential for Fabric Cosmos DB access with automatic refresh and retry logic.\"\"\"\n",
    "    \n",
    "    def get_token(self, *scopes: str, claims: Optional[str] = None, tenant_id: Optional[str] = None,\n",
    "                  enable_cae: bool = False, **kwargs: Any) -> AccessToken:\n",
    "        access_token = notebookutils.credentials.getToken(\"https://cosmos.azure.com/.default\")\n",
    "        parts = access_token.split(\".\")\n",
    "        if len(parts) < 2:\n",
    "            raise ValueError(\"Invalid JWT format\")\n",
    "        payload_b64 = parts[1]\n",
    "        # Fix padding\n",
    "        padding = (-len(payload_b64)) % 4\n",
    "        if padding:\n",
    "            payload_b64 += \"=\" * padding\n",
    "        payload_json = base64.urlsafe_b64decode(payload_b64.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "        payload = json.loads(payload_json)\n",
    "        exp = payload.get(\"exp\")\n",
    "        if exp is None:\n",
    "            raise ValueError(\"exp claim missing in token\")\n",
    "        return AccessToken(token=access_token, expires_on=exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a108c3-990d-45fb-ad39-0271b9aa9bee",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Initialize Cosmos DB Clients ###\n",
    "\n",
    "Create clients for the database and containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec6d7a-f979-446c-bf40-e545a0d1f928",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Cosmos DB cosmos client\n",
    "COSMOS_CLIENT = CosmosClient(COSMOS_ENDPOINT, FabricTokenCredential())\n",
    "\n",
    "# Initialize Cosmos DB database client\n",
    "DATABASE_CLIENT = COSMOS_CLIENT.get_database_client(COSMOS_DATABASE_NAME)\n",
    "\n",
    "# Intialize Cosmos DB container client\n",
    "txns_container = DATABASE_CLIENT.get_container_client(COSMOS_TRANSACTION_CONTAINER_NAME) \n",
    "card_container = DATABASE_CLIENT.get_container_client(COSMOS_CC_CONTAINER_NAME)\n",
    "pending_txns_container = DATABASE_CLIENT.get_container_client(COSMOS_PENDING_TRANSACTION_CONTAINER_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a313eebc-f37b-488c-a1d4-9e221684edc8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Weights per your formula\n",
    "W_AMOUNT   = 0.2\n",
    "W_MERCHANT = 0.3\n",
    "W_LOCATION = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d1fcc5-f800-4295-97e3-6525cb4b78dc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Embedding Helper\n",
    "Wraps a call to the embeddings API and returns a NumPy vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6405c876-0419-4ceb-b57c-f2a879ed098a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "# ─────────────────────────────────────────────\n",
    "# Embedding helper\n",
    "# ─────────────────────────────────────────────\n",
    "def embed_text(text: str) -> np.ndarray:\n",
    "    resp = openai.embeddings.create(input=text, model=OPEN_AI_MODEL)\n",
    "    return np.array(resp.data[0].embedding, dtype=np.float32)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=5000)\n",
    "def embed_text_cached(text: str) -> np.ndarray:\n",
    "    return embed_text(text)\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# Combine embedding (amount + merchant + location)\n",
    "# ─────────────────────────────────────────────\n",
    "\n",
    "def normalize_amount(amount: float, lo: float, hi: float) -> float:\n",
    "    span = max(hi - lo, 1e-6)\n",
    "    return float(np.clip((amount - lo) / span, 0.0, 1.0))\n",
    "\n",
    "def make_embedding(merchant: str, location: str, amount: float, lo: float, hi: float) -> list:\n",
    "    amount_norm = normalize_amount(amount, lo, hi)\n",
    "    a_vec = np.array([amount_norm], dtype=np.float32) * W_AMOUNT\n",
    "    m_vec = embed_text(merchant) * W_MERCHANT\n",
    "    l_vec = embed_text(location) * W_LOCATION\n",
    "    combined = np.concatenate([a_vec, m_vec, l_vec]).astype(np.float32)\n",
    "    norm = np.linalg.norm(combined)\n",
    "    if norm > 0:\n",
    "        combined /= norm\n",
    "    return combined.tolist()\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# Generate and insert a transaction\n",
    "# ─────────────────────────────────────────────\n",
    "def add_transaction(card_id: str, customer_id: str, merchant: str, location: str,\n",
    "                    amount: float, lo: float, hi: float):\n",
    "    emb = make_embedding(merchant, location, amount, lo, hi)\n",
    "    doc = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"type\": \"transaction\",\n",
    "        \"card_id\": card_id,\n",
    "        \"customer_id\": customer_id,\n",
    "        \"merchant\": merchant,\n",
    "        \"location\": location,\n",
    "        \"amount\": amount,\n",
    "        \"embedding\": emb,\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6975b0-da9f-4ff7-b5da-3cf1f404db47",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Fraud Detection Logic Overview ###\n",
    "The is_fraudulent_transaction function implements a vector-based anomaly detection approach scoped to a single credit card. It uses historical transaction embeddings to determine whether a new transaction is suspicious.\n",
    "\n",
    "**Step-by-Step Process**\n",
    "\n",
    "\n",
    "- **Provisional Amount Band**\n",
    "\n",
    "    - Compute an initial (lo, hi) range for the transaction amount using _provisional_band. \n",
    "    - This stabilizes normalization before we have enough historical neighbors.\n",
    "\n",
    "\n",
    "\n",
    "- **Generate Provisional Embedding**\n",
    "\n",
    "    - Build an embedding for the candidate transaction using merchant, location, amount, and the provisional band. \n",
    "    - Convert to list[float] for Cosmos DB vector query compatibility.\n",
    "\n",
    "\n",
    "\n",
    "- **Fetch Nearest Neighbors**\n",
    "\n",
    "    - Query the CCTransactions container for TOP-K vector neighbors within the same card partition using VectorDistance.\n",
    "    - No merchant restriction; purely card-wide similarity.\n",
    "\n",
    "\n",
    "\n",
    "- **Refine Amount Band**\n",
    "\n",
    "    - If neighbors exist, compute a robust (lo, hi) band using P5 and P95 percentiles of historical amounts.\n",
    "    - Fallback to provisional band if no neighbors.\n",
    "\n",
    "\n",
    "\n",
    "- **Rebuild Embedding**\n",
    "\n",
    "    - Generate a new embedding using the refined band for better normalization.\n",
    "\n",
    "\n",
    "\n",
    "- **Prepare Neighbor Embeddings**\n",
    "    - Collect embeddings from neighbors into a NumPy array for distance calculations.\n",
    "\n",
    "\n",
    "\n",
    "- **Decision Logic**\n",
    "\n",
    "    - If fewer than 5 historical transactions exist, return False (not enough data to judge).\n",
    "    - Compute the centroid of neighbor embeddings and a dynamic threshold:\n",
    "        \n",
    "        - Centroid Calculation\n",
    "            $$\n",
    "                \\text{centroid} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{x}_i\n",
    "            $$\n",
    "        - Distance for each neighbor\n",
    "            $$\n",
    "                d_i = \\|\\mathbf{x}_i - \\text{centroid}\\|_2\n",
    "            $$\n",
    "        - Threshold Formula\n",
    "            $$\n",
    "                \\text{threshold} = \\mu_d + (\\text{multiplier} \\times \\sigma_d)\n",
    "            $$\n",
    "        - Fraud Decision Rule\n",
    "            $$\n",
    "                \\|\\mathbf{x}_{\\text{new}} - \\text{centroid}\\|_2 > \\text{threshold}\n",
    "            $$\n",
    "        - Provisional Band\n",
    "            $$\n",
    "                \\text{span} = 10.0 + 0.25 \\cdot \\log(1 + a) \\cdot a^{0.25}\n",
    "            $$\n",
    "            $$\n",
    "                \\text{lo} = \\max(0.01, a - \\text{span}), \\quad \\text{hi} = a + \\text{span}\n",
    "            $$\n",
    "        \n",
    "\n",
    "    - Calculate the L2 distance of the new embedding to the centroid.\n",
    "    - **Flag as fraud if**:\n",
    "       - $$\n",
    "            \\text{new\\_dist} > \\text{threshold}\n",
    "         $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b016a99a-d6cc-4773-9d55-714b63646c9a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Boolean-only fraud check using card-wide vector neighbors (no merchant filter)\n",
    "# Returns: True if anomalous/fraud, False otherwise\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "def _provisional_band(amount: float) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Cold-start band so amount normalization is stable before we have neighbors.\n",
    "    Same shape as your earlier fallback; gentle growth with amount.\n",
    "    \"\"\"\n",
    "    a = max(1.0, float(amount))\n",
    "    span = 10.0 + 0.25 * np.log1p(a) * a ** 0.25\n",
    "    lo = max(0.01, a - span)\n",
    "    hi = a + span\n",
    "    return float(lo), float(hi)\n",
    "\n",
    "def _vector_neighbors_for_card(\n",
    "    *,\n",
    "    card_id: str,\n",
    "    query_embedding: List[float],\n",
    "    k: int = 100\n",
    ") -> List:\n",
    "    \"\"\"\n",
    "    Returns up to K nearest neighbors by cosine distance from the given query embedding,\n",
    "    partition-scoped to the card (no merchant restriction).\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT TOP @k c.embedding, c.amount, c.timestamp\n",
    "    FROM c\n",
    "    WHERE c.card_id = @card\n",
    "      AND c.type = 'transaction'\n",
    "    ORDER BY VectorDistance(c.embedding, @emb)\n",
    "    \"\"\"\n",
    "    params = [\n",
    "        {\"name\": \"@k\",    \"value\": int(k)},\n",
    "        {\"name\": \"@card\", \"value\": card_id},\n",
    "        {\"name\": \"@emb\",  \"value\": query_embedding},\n",
    "    ]\n",
    "    return list(txns_container.query_items(\n",
    "        query=query,\n",
    "        parameters=params,\n",
    "        partition_key=card_id,\n",
    "        enable_cross_partition_query=False\n",
    "    ))\n",
    "\n",
    "def _robust_amount_band_from_neighbors(neighbors: List[Dict]) -> Optional[Tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Robust (lo, hi) from neighbor amounts using percentiles (P5, P95).\n",
    "    \"\"\"\n",
    "    if not neighbors:\n",
    "        return None\n",
    "    amts = [float(n[\"amount\"]) for n in neighbors if \"amount\" in n]\n",
    "    if not amts:\n",
    "        return None\n",
    "    lo = float(np.percentile(amts, 5))\n",
    "    hi = float(np.percentile(amts, 95))\n",
    "    if hi <= lo:\n",
    "        hi = lo + max(5.0, 0.1 * max(1.0, lo))\n",
    "    return lo, hi\n",
    "\n",
    "def _centroid_threshold(\n",
    "    embeddings: np.ndarray,\n",
    "    multiplier: float = 2.5\n",
    ") -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Centroid and threshold = mean(dist) + k * std(dist),\n",
    "    where dist is L2 distance to centroid.\n",
    "    \"\"\"\n",
    "    centroid = embeddings.mean(axis=0)\n",
    "    dists = np.linalg.norm(embeddings - centroid, axis=1)\n",
    "    mu = float(np.mean(dists))\n",
    "    sigma = float(np.std(dists))\n",
    "    return centroid, (mu + multiplier * sigma)\n",
    "\n",
    "def is_fraudulent_transaction(\n",
    "    *,\n",
    "    card_id: str,\n",
    "    merchant: str,\n",
    "    location: str,\n",
    "    amount: float,\n",
    "    k_neighbors: int = 50,\n",
    "    multiplier: float = 2.5\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the candidate transaction is anomalous/fraudulent, else False.\n",
    "    No writes. No merchant restriction. Single vector TOP-K query (partition-scoped).\n",
    "    \"\"\"\n",
    "    # 1) Provisional band → stable normalization before neighbors exist\n",
    "    prov_lo, prov_hi = _provisional_band(amount)\n",
    "\n",
    "    # 2) Provisional embedding for the vector search\n",
    "    new_emb_prov = np.asarray(\n",
    "        make_embedding(merchant=merchant, location=location, amount=float(amount), lo=prov_lo, hi=prov_hi),\n",
    "        dtype=np.float32\n",
    "    ).tolist()  # Cosmos SQL param expects list[float]\n",
    "\n",
    "    # 3) Fetch TOP-K neighbors within the card partition\n",
    "    neighbors = _vector_neighbors_for_card(\n",
    "        card_id=card_id,\n",
    "        query_embedding=new_emb_prov,\n",
    "        k=k_neighbors\n",
    "    )\n",
    "\n",
    "    # 4) Derive a refined (lo, hi) band from neighbors; fallback to provisional if none\n",
    "    band = _robust_amount_band_from_neighbors(neighbors)\n",
    "    lo, hi = (band if band is not None else (prov_lo, prov_hi))\n",
    "\n",
    "    # 5) Rebuild embedding using the refined (lo, hi)\n",
    "    new_emb = np.asarray(\n",
    "        make_embedding(merchant=merchant, location=location, amount=float(amount), lo=lo, hi=hi),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    \n",
    "    # 6) Prepare neighbor embeddings\n",
    "    vecs = []\n",
    "    for n in neighbors:\n",
    "        emb = n.get(\"embedding\")\n",
    "        if isinstance(emb, list) and emb:\n",
    "            vecs.append(np.asarray(emb, dtype=np.float32))\n",
    "    hist = np.vstack(vecs) if vecs else np.empty((0, 0), dtype=np.float32)\n",
    "\n",
    "    # 7) Decision: if not enough history, do NOT flag (return False)\n",
    "    if hist.size == 0 or hist.shape[0] < 5:\n",
    "        return False\n",
    "\n",
    "    centroid, threshold = _centroid_threshold(hist, multiplier=multiplier)\n",
    "    new_dist = float(np.linalg.norm(new_emb - centroid))\n",
    "    return new_dist > threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaa7d18-0c32-4356-bc24-8494fc213bfc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Quick Fraud Check ####\n",
    "A Quick check to verify fraud detection is working with Cosmos Containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae17bc2-e8a8-4ffa-86c0-9d690c783643",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "is_anom = is_fraudulent_transaction(\n",
    "    card_id=\"C0001\",\n",
    "    merchant=\"Samsung\",\n",
    "    location=\"Rhode Island\",\n",
    "    amount=180.00,\n",
    "    k_neighbors=32,\n",
    "    multiplier=2.5\n",
    ")\n",
    "print(\"Transaction for Customer 1 _is_fraudulent:\", is_anom)\n",
    "\n",
    "is_anom = is_fraudulent_transaction(\n",
    "    card_id=\"C0002\",\n",
    "    merchant=\"Subway\",\n",
    "    location= \"California\",\n",
    "    amount= 30.00,\n",
    "    k_neighbors=32,\n",
    "    multiplier=2.5)\n",
    "\n",
    "print(\"Transaction for Customer 2 _is_fraudulent:\", is_anom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff02d5e-7aef-456c-9a40-e60856a2d6d1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Change Feed Parsing and Schema Enforcement ###\n",
    "\n",
    "- **Define Transaction Schema** as it should match the expected values from the pending transaction items.\n",
    "\n",
    "- **Read Change Feed Stream**: We use the Cosmos DB Spark connector to read the incremental change feed from the COSMOS_PENDING_TRANSACTION_CONTAINER_NAME container.\n",
    "\n",
    "- **Parse Raw Body**: \n",
    "    - Parse _rawBody JSON into structured columns using from_json and txn_schema.\n",
    "    - Preserve metadata (id, _ts, _lsn, _etag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93738b88-04c5-475d-bef7-a4915b96aa75",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, struct, to_json, from_json\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType, TimestampType\n",
    "\n",
    "\n",
    "txn_schema = StructType([\n",
    "    StructField(\"id\", StringType()),                # Document ID\n",
    "    StructField(\"type\", StringType()),              # e.g., \"transaction\"\n",
    "    StructField(\"card_id\", StringType()),\n",
    "    StructField(\"customer_id\", StringType()),\n",
    "    StructField(\"merchant\", StringType()),\n",
    "    StructField(\"location\", StringType()),\n",
    "    StructField(\"amount\", DoubleType()),\n",
    "    StructField(\"timestamp\", StringType())\n",
    "])\n",
    "\n",
    "\n",
    "changefeed_df: DataFrame = (\n",
    "    spark.readStream\n",
    "         .format(\"cosmos.oltp.changeFeed\")\n",
    "         .options(**{\n",
    "             **base_read_cfg,\n",
    "             \"spark.cosmos.container\": COSMOS_PENDING_TRANSACTION_CONTAINER_NAME,\n",
    "             \"spark.cosmos.changeFeed.mode\": \"Incremental\",\n",
    "             \"spark.cosmos.changeFeed.startFrom\": \"Now\",\n",
    "         })\n",
    "         .load()\n",
    ")\n",
    "\n",
    "# Filter tombstones if column exists (schema can vary)\n",
    "cf_cols = set(changefeed_df.columns)\n",
    "if \"_isDelete\" in cf_cols:\n",
    "    changefeed_df = changefeed_df.filter(col(\"_isDelete\") == False)\n",
    "\n",
    "incoming_df = (\n",
    "    changefeed_df\n",
    "      .select(\n",
    "          from_json(col(\"_rawBody\").cast(\"string\"), txn_schema).alias(\"doc\"),\n",
    "          col(\"id\"),\n",
    "          col(\"_ts\"),\n",
    "          col(\"_lsn\"),\n",
    "          col(\"_etag\"),\n",
    "      )\n",
    "      .select(\"doc.*\", \"id\", \"_ts\", \"_lsn\", \"_etag\")\n",
    ")\n",
    "\n",
    "# Ensure essential fields exist\n",
    "cf_cols = set(incoming_df.columns)\n",
    "required_cols = [\"id\", \"card_id\", \"customer_id\", \"merchant\", \"location\", \"amount\"]\n",
    "for c in required_cols:\n",
    "    if c not in cf_cols:\n",
    "        raise ValueError(f\"Expected column '{c}' not found in change feed schema: {sorted(cf_cols)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb094b82-3b7a-49bb-9233-992a60bff56f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### foreachBatch for Fraud Detection and Card Locking ###\n",
    "\n",
    "This step processes each micro-batch from the Cosmos DB change feed. It performs three key actions:\n",
    "\n",
    "- **Lock Check**: Before any fraud logic runs, the code queries the CreditCards container to skip transactions for cards already marked as locked. It prints the lock reason and timestamp.\n",
    "- **Fraud Detection**: For unlocked cards, it calls is_fradulent_transaction to determine if the transaction is anomalous based on historical embeddings.\n",
    "- **Conditional Actions**:\n",
    "    - If fraudulent, Patch the card status to locked and record metadata.\n",
    "    - If clean, Upsert the transaction into the CCTransactions container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a87d48-7dba-411d-bba0-bdb9bfbc8b38",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Consolidated foreachBatch with your FabricTokenCredential and controlled concurrency\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def _process_batch(batch_df, batch_id: int):\n",
    "    from azure.cosmos import CosmosClient\n",
    "\n",
    "    client = COSMOS_CLIENT\n",
    "    cards  = card_container     # for patching\n",
    "    cc_tx  = txns_container      # for neighbor search & inserts\n",
    "\n",
    "    now_iso = datetime.now(timezone.utc).isoformat()\n",
    "    patch_ops = [\n",
    "        {\"op\": \"set\", \"path\": \"/status\",            \"value\": \"locked\"},\n",
    "        {\"op\": \"set\", \"path\": \"/last_lock_reason\", \"value\": \"locked for fraudulent transaction\"},\n",
    "        {\"op\": \"set\", \"path\": \"/last_updated\",     \"value\": now_iso},\n",
    "    ]\n",
    "\n",
    "    rows = list(batch_df.toLocalIterator())\n",
    "    if not rows:\n",
    "        print(f\"[Batch {batch_id}] No rows.\")\n",
    "        return\n",
    "\n",
    "    def handle_row(r) -> tuple[bool, str]:\n",
    "        d = r.asDict(recursive=True)\n",
    "        card_id     = d.get(\"card_id\")\n",
    "        customer_id = d.get(\"customer_id\")\n",
    "        merchant    = d.get(\"merchant\")\n",
    "        location    = d.get(\"location\")\n",
    "        amount      = float(d.get(\"amount\", 0.0))\n",
    "\n",
    "        if not card_id or not merchant or not location:\n",
    "            return (False, \"skip-missing-fields\")\n",
    "\n",
    "        \n",
    "        if not customer_id:\n",
    "            return (False, \"skip-missing-customer-id\")\n",
    "\n",
    "        lock_check_query = \"\"\"\n",
    "            SELECT TOP 1 c.id, c.status, c.last_lock_reason, c.last_updated\n",
    "            FROM c\n",
    "            WHERE c.customer_id = @cid AND c.card_id = @card\n",
    "        \"\"\"\n",
    "        lock_check_params = [{\"name\": \"@cid\", \"value\": customer_id},\n",
    "                             {\"name\": \"@card\", \"value\": card_id}]\n",
    "        try:\n",
    "            lock_docs = list(cards.query_items(\n",
    "                query=lock_check_query,\n",
    "                parameters=lock_check_params,\n",
    "                partition_key=customer_id,\n",
    "                enable_cross_partition_query=False\n",
    "            ))\n",
    "            if lock_docs:\n",
    "                card_meta = lock_docs[0]\n",
    "                if str(card_meta.get(\"status\", \"\")).lower() == \"locked\":\n",
    "                    reason = card_meta.get(\"last_lock_reason\", \"(no reason recorded)\")\n",
    "                    when   = card_meta.get(\"last_updated\", \"(no timestamp recorded)\")\n",
    "                    print(f\"[Batch {batch_id}] Card {card_id} is LOCKED — reason: {reason} — last_updated: {when}. Skipping transaction.\")\n",
    "                    return (False, \"skip-locked\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] lock check failed (cust={customer_id}, card={card_id}): {e}\")\n",
    "            # Conservative choice: if we can't verify lock status, continue to normal flow\n",
    "            return (False, \"lock-check-error\")\n",
    "\n",
    "\n",
    "        # Decide\n",
    "        try:\n",
    "            is_fraud = is_fraudulent_transaction(\n",
    "                card_id=card_id,\n",
    "                merchant=merchant,\n",
    "                location=location,\n",
    "                amount=amount,\n",
    "                k_neighbors=50,          # start smaller for RU efficiency\n",
    "                multiplier=2.5         \n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] decision error (card={card_id}): {e}\")\n",
    "            return (False, \"decision-error\")\n",
    "\n",
    "        if is_fraud:\n",
    "            # Patch CreditCards inside the customer partition\n",
    "            print(f\"[Batch {batch_id}] Fraud detected for card {card_id} at merchant {merchant}, location {location}\")\n",
    "            if not customer_id:\n",
    "                return (False, \"fraud-no-customer-id\")\n",
    "            # Query IDs within partition\n",
    "            query = \"\"\"\n",
    "                SELECT c.id\n",
    "                FROM c\n",
    "                WHERE c.customer_id = @cid AND c.card_id = @card\n",
    "            \"\"\"\n",
    "            params = [{\"name\": \"@cid\", \"value\": customer_id}, {\"name\": \"@card\", \"value\": card_id}]\n",
    "            try:\n",
    "                results = list(cards.query_items(\n",
    "                    query=query,\n",
    "                    parameters=params,\n",
    "                    partition_key=customer_id,\n",
    "                    enable_cross_partition_query=False\n",
    "                ))\n",
    "                for doc in results:\n",
    "                    # patch with retry for 429\n",
    "                    delay = 0.5\n",
    "                    for _ in range(6):\n",
    "                        try:\n",
    "                            cards.patch_item(item=doc[\"id\"], partition_key=customer_id, patch_operations=patch_ops)\n",
    "                            break\n",
    "                        except Exception as ex:\n",
    "                            msg = str(ex).lower()\n",
    "                            if \"429\" in msg or \"rate is large\" in msg:\n",
    "                                time.sleep(delay); delay = min(delay*2, 8.0); continue\n",
    "                            print(f\"[WARN] patch failed (cust={customer_id}, card={card_id}): {ex}\")\n",
    "                            break\n",
    "                print(f\"[Batch {batch_id}] Card {card_id} has been locked due to fraudulent transaction\")\n",
    "                return (True, \"patched\")\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] patch query failed (cust={customer_id}, card={card_id}): {e}\")\n",
    "                return (False, \"patch-error\")\n",
    "        else:\n",
    "            # Upsert the clean transaction into CCTransactions (partition key = card_id)\n",
    "            try:\n",
    "                # Ensure required fields present and consistent casing\n",
    "                d[\"customer_id\"] = customer_id\n",
    "                d.pop(\"partition_key\", None)\n",
    "                cc_tx.upsert_item(d)\n",
    "                print(f\"[Batch {batch_id}] Transaction upserted for card {card_id}\")\n",
    "                return (True, \"upserted\")\n",
    "            except Exception as e:\n",
    "                msg = str(e).lower()\n",
    "                if \"429\" in msg or \"rate is large\" in msg:\n",
    "                    # simple retry\n",
    "                    delay = 0.5\n",
    "                    for _ in range(6):\n",
    "                        try:\n",
    "                            cc_tx.upsert_item(d)\n",
    "                            print(f\"[Batch {batch_id}] Transaction upserted for card {card_id}\")\n",
    "                            return (True, \"upserted-retry\")\n",
    "                        except Exception as ex:\n",
    "                            if \"429\" in str(ex).lower():\n",
    "                                time.sleep(delay); delay = min(delay*2, 8.0); continue\n",
    "                            print(f\"[WARN] upsert failed (card={card_id}): {ex}\")\n",
    "                            break\n",
    "                else:\n",
    "                    print(f\"[WARN] upsert failed (card={card_id}): {e}\")\n",
    "                return (False, \"upsert-error\")\n",
    "\n",
    "    # I/O-bound → safe to use threads\n",
    "    max_workers = min(32, max(4, len(rows)))\n",
    "    ok = 0\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = [ex.submit(handle_row, r) for r in rows]\n",
    "        for f in as_completed(futures):\n",
    "            success, _ = f.result()\n",
    "            ok += 1 if success else 0\n",
    "\n",
    "    print(f\"[Batch {batch_id}] processed={len(rows)} , success={ok}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937525b3-6454-435f-b340-ae5783aa6acd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "consolidated_query = (\n",
    "    incoming_df   # from your change feed read\n",
    "      .writeStream\n",
    "      .foreachBatch(_process_batch)\n",
    "      .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7319f5-cc06-4529-ab43-84bf81e8be32",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Monitor & control (single consolidated stream)\n",
    "print(\"Consolidated query id:\", consolidated_query.id)\n",
    "consolidated_query.awaitTermination()\n",
    "\n",
    "# Optional helpers:\n",
    "# consolidated_query.stop()\n"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "environment": {
    "environmentId": "3573362e-c15c-404e-bf55-922ac85c83ec",
    "workspaceId": "f0c2894f-8cb2-413a-9eb8-f1afb4ea5dea"
   },
   "lakehouse": null
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "language": null,
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
